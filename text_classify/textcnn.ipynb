{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input,Conv2D,MaxPool2D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "import yaml\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    #数据集路径\n",
    "    dataSource = \"../marriage.txt\"\n",
    "    stopWordSource = \"../stopword.txt\"\n",
    "    #分词后保留大于等于最低词频的词\n",
    "    miniFreq=0\n",
    "    #统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n",
    "    sequenceLength = 20 \n",
    "    batchSize=128\n",
    "    epochs=20\n",
    "    numClasses = 5\n",
    "    #训练集的比例\n",
    "    rate = 0.8  \n",
    "    #生成嵌入词向量的维度\n",
    "    embeddingSize = 200\n",
    "    #卷积核数\n",
    "    numFilters = 128\n",
    "    #卷积核大小\n",
    "    filterSizes = [1,2,3,4,5,6]\n",
    "    dropoutKeepProb = 0.5\n",
    "    #L2正则系数\n",
    "    l2RegLambda = 0.01\n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文语料\n",
    "#设置输出日志\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "file = open(\"../marriage.txt\") \n",
    "sentences=[]\n",
    "with open('../marriage.txt') as fr:\n",
    "    for line in fr.readlines():\n",
    "        temp=line.strip().split('\\t')\n",
    "#         sentences.append(jieba.lcut(temp[0]))\n",
    "        sentences.append(list(temp[0]))\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,size=config.embeddingSize,\n",
    "                     min_count=config.miniFreq,\n",
    "                     window=10,\n",
    "                     workers=multiprocessing.cpu_count(),sg=1,\n",
    "                     iter=20)\n",
    "model.save('../word2VecModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['你', '坤', '包', '好', '了', '啊', '说', '回', '信', '我', '现', '在', '发', '也', '不', '方', '便', '还', '会', '只', '是', '随', '注', '册', '一', '下', 'm', '特', '意', '去', '那', '个', '嘞', '登', '记', '过', '吗', '给', '就', '想', '珍', '爱', '网', '上', '面', '这', '样', '的', '谢', '像', '天', '有', '人', '联', '系', '用', '吧', '之', '前', '边', '跟', '嗯', '太', '改', '再', '班', '呢', '没', '大', '几', '年', '怎', '么', '办', '七', '二', '看', '反', '正', '然', '后', '知', '道', '载', '完', '高', '呃', '什', '证', '件', '但', '豪', '门', '哦', '行', '刚', '才', '点', '事', '先', '因', '为', '们', '暂', '时', '需', '要', '服', '务', '分', '钟', '菜', '家', '里', '外', '啥', '思', '呀', '早', '肯', '定', '久', '电', '话', '离', '婚', '法', '院', '判', '决', '书', '共', '取', '消', '确', '实', '忙', '等', '小', '孩', '弄', '手', '机', '告', '稿', '贾', '易', '玲', '喂', '理', '英', '文', '字', '麻', '烦', '拜', '升', '级', '选', '打', '钱', '工', '地', '八', '号', '咋', '复', '印', '出', '来', '头', '拿', '到', '三', '错', '干', '男', '学', '校', '稍', '微', '开', '对', '订', '都', '周', '六', '核', '他', '准', '备', '汤', '期', '四', '今', '候', '爸', '拌', '挂', '重', '新', '以', '扰', '视', '频', '掉', '呗', '聊', '楼', '老', '身', '吃', '见', '月', '十', '其', '快', '递', '休', '息', '晚', '诶', '报', '把', '名', '能', '谈', '朋', '友', '直', '已', '经', '多', '啦', '空', '哪', '走', '马', '礼', '卸', '帮', '着', '急', '车', '牌', '从', '觉', '得', '击', '种', '嘛', '放', '例', '迁', '移', '子', '美', '女', '故', '换', '江', '西', '感', '解', '算', '介', '绍', '很', '清', '楚', '钥', '匙', '希', '望', '明', '哎', '些', '棒', '找', '指', '当', '听', '未', '妈', '零', '如', '果', '白', '云', '焊', '接', '结', '困', '问', '姐', '资', '料', '善', '两', '万', '关', '又', '拼', '音', '销', '毁', '爆', '喜', '欢', '态', '度', '死', '玩', '东', '沟', '通', '妥', '带', '睡', '维', '权', '咯', '份', '王', '课', '答', '题', '怪', '淘', '宝', '变', '历', '忘', '目', '最', '情', '银', '假', '间', '北', '团', '密', '码', '婆', '午', '撤', '您', '可', '中', '照', '华', '生', '活', '送', '进', '搞', '拨', '百', '强', '壮', '吵', '卡', 'y', 'a', ' ', '双', '做', '作', '段', '或', '者', '风', '国', '内', '兴', '填', '别', '丢', '片', '单', '既', '跑', '加', '入', '提', '旧', '版', '断', '应', '该', '奇', '科', '温', '州', '瑞', '安', '饭', '哈', '台', '九', '供', '真', '咨', '询', '五', '所', '儿', '商', '户', '翻', '盘', '拍', '买', '谁', '修', '语', '本', '相', '亲', '呐', '少', '南', '昌', '次', '无', '搬', '动', '笑', '陆', '充', '值', '账', '示', '灯', '合', '战', '玛', '杜', '传', '懂', '讲', '卷', '民', '政', '豆', '瓣', '每', '局', '湿', '长', '套', '坐', '口', '京', '客', '考', '虑', '叫', '员', '贵', '昨', '址', '幺', '邮', '扔', '含', '流', '程', '收', '汉', '旅', '游', '申', '请', '删', '协', '议', '形', '成', '配', '偶', '替', '妹', '雨', '奖', '励', '赶', '货', '案', '坏', '监', '控', '律', '笔', '半', '和', '张', '翰', '按', '键', '徐', '领', '留', 'v', 'i', 'p', '牙', '医', '火', '公', '青', '春', '审', '评', '价', '姓', '丁', '晕', '丧', '势', '挣', '自', '己', '签', '路', '香', '港', '骑', '海', '型', '府', '乐', '整', '常', '鬼', '李', '深', '预', '扣', '利', '润', '饿', '抢', '油', '咱', '交', '房', '而', '且', '代', '认', 'w', 'o', 'r', 'd', '连', '平', '寄', '简', '查', '添', '肛', '操', '象', '厕', '箱', '旁', '受', '横', '同', '写', '伙', '链', '千', '类', '搜', '索', '显', '装', '被', '骗', '材', '声', '纠', '纷', '锋', '阵', '试', '毕', '竟', '验', '称', '傍', '圈', '戚', '板', '起', '诉', '于', '洪', '差', '4', '0', '碌', '哇', '嗳', '舞', '蹈', '虽', '师', '喽', '居', '聪', '纯', '脑', '具', '任', '何', '蚊', '红', '乱', '误', '炒', '区', '涂', '晓', '使', '噢', '处', '纸', '灵', '魂', '站', '唉', '举', '灾', '沙', '全', '烧', '枣', '担', '心', '况', '秋', '节', '设', '线', '疑', '录', '呦', '失', '嫁', '保', '佑', '般', '广', '姚', '转', '脚', '派', 'k', '部', '耶', '绑', '根', '俊', '星', '另', '占', '投', '图', '遍', '司', '远', '量', '售', '秒', '杰', '票', '怕', '参', '基', '赔', '舒', '戏', '器', '抱', '条', '往', '据', '求', '退', '醒', '卖', '限', '智', '向', '欠', '呼', '溜', '冰', '姥', '难', '更', '惨', '阴', '日', '毛', '块', '比', '必', '须', '吕', '舔', '她', '教', '软', '哥', '位', '花', '梅', '脸', '股', '光', '众', '征', '功', '混', '拔', '草', '让', '概', '狗', '论', '始', '速', '款', '补', '艺', '益', '促', '澡', '巾', '嫂', '力', '呵', '胖', '待', '压', '振', '框', '志', '排', '原', '适', '穿', '范', '置', '牛', '杠', '芦', '荟', '胶', '父', '尔', '由', '管', '底', '令', '较', '它', '薄', '唱', '第', '攻', '闭', '卫', '辉', '歉', '室', '束', '兆', '挖', '掘', 's', '企', '业', '立', '购', '效', '唤', '湾', '属', '害', '冯', '燕', '朵', '貌', '似', '铁', '累', '闲', '访', '优', '惠', '甘', '肃', '怀', '念', '建', '浓', '涨', '宽', '列', '庭', '嗨', '触', '哼', '颁', '元', '刘', '输', '蛋', '雁', '顺', '绿', '化', '番', '赚', '黄', '鸭', '总', '庆', '浩', '导', '致', '首', '伴', '凌', '晨', '借', '贷', '杭', '金', '额', '士', '霉', '存', '险', '凉', '昏', '僭', '近', '圳', '满', '层', '郁', '闷', '绝', '挺', '洗', '域', '闺', '哟', '猫', '影', '响', '各', '陈', '倩', '岁', '嘟', '弹', '扯', '淡', '体', '谅', '妞', '播', '滚', '碰', '闻', '顾', '病', '罢', '妆', '与', '噻', '侍', '达', '费', '场', '迟', '抓', '紧', '乳', '阶', '支', '持', '吞', '噬', '城', '康', '梨', '摆', '眼', '启', '杂', '峰', '镇', '运', '气', '遇', '刻', '调', '疯', '主', '章', '赵', '林', '朝', '凤', '市', '米', '遭', '尽', '越', '浪', '厅', '亮', '初', '鸡', '巴', '嚎', '创', '逃', '避', '厂', '努', '推', '荐', '逼', '咽', '浏', '览', '括', '践', '奢', '侈', '神', '左', '右', '斗', '罗', '雅', '质', '耽', '搁', '冻', '凭', '迷', '靠', '付', '统', '计', '夜', '缘', '临', '汾', '婉', '患', '式', 'q', '检', '吓', '跳', '续', '桂', '捺', '住', '异', '鸿', '伯', '店', '架', '洲', '余', '标', '阿', '震', '诞', '童', '铺', '伟', '队', '端', '卦', '裁', '咸', '官', '鞋', '齐', '栋', '估', '扎', '拉', '蛮', '石', '庄', '丶', '并', '汕', '夏', '羞', '炮', '品', '尾', '物', '帐', '农', '水', '讼', '斥', '探', '讨', '逗', '际', '招', '嘲', '锁', '表', '苏', '宁', '烟', '颜', '蚂', '蚁', '骂', '析', '雯', '辛', '苦', '阳', '鸣', '烊', '药', '瓦', '盖', '泵', '状', '粘', '静', '拥', '黎', '宏', '废', '档', '拖', '昆', '扮', '姻', '皆', '除', '散', '划', '岭', '沈', '扫', '倒', '偷', '独', '透', '露', '性', '黑', '山', '厉', '骏', '兜', '策', '欧', '读', '慢', '纪', '呆', '尺', '寸', '良', '梦', '均', '刷', '隐', '私', '积', '樱', '娃', '批', '摄', '拾', '铃', '凰', '愿', '遗', '够', '邻', '研', '究', '斩', '详', '细', '轻', '及', '犯', '仔', '骚', '贩', '丰', '县', '邓', '绩', '娘', '丽', '馈', '甲', '伤', '谎', '言', '瞬', '撑', '袋', '助', '省', '荤', '术', '诈', '泰', '座', '杨', '狂', '猜', '娜', '贴', '膜', '钻', '捎', '纤', '赖', '魔', '糟', '劝', '歌', '尝', '增', '匹', '曾', '测', '糖', '革', '柿', '愤', '布', '圣', '剑', '释', '傻', '映', '约', '盐', '孙', '败', '观', '专', '世', '追', '谓', '湖', '渠', '球'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.62259936e-01, -1.01414181e-01, -9.01771933e-02,  1.38697177e-01,\n",
       "       -2.15469211e-01,  1.72081217e-01,  8.95420536e-02,  8.58433470e-02,\n",
       "        2.62835175e-01, -4.46709767e-02, -2.66334944e-04, -2.31834218e-01,\n",
       "        1.01651862e-01, -1.06230192e-01,  1.49743512e-01, -2.03052536e-01,\n",
       "        1.66971967e-01, -4.97035757e-02,  6.66131973e-02,  2.21493095e-01,\n",
       "        9.60768759e-02, -3.17073101e-03, -1.34310842e-01,  2.14327380e-01,\n",
       "        2.85391323e-02,  7.95748979e-02, -1.83805019e-01,  9.30562764e-02,\n",
       "        1.01410877e-03, -1.53992116e-01,  2.69832730e-01,  1.91341475e-01,\n",
       "        3.01492512e-01,  1.03915900e-01,  9.62394550e-02,  1.12453088e-01,\n",
       "       -2.89852589e-01,  7.69588426e-02, -1.02878004e-01, -2.65263289e-01,\n",
       "        5.85740097e-02, -4.51844901e-01,  5.51657341e-02,  1.36977777e-01,\n",
       "       -1.17409185e-01,  1.56495303e-01, -3.65037084e-01,  1.15589097e-01,\n",
       "       -3.55736136e-01, -1.35263637e-01, -1.09639261e-02,  5.38873255e-01,\n",
       "        3.13623846e-01,  4.64130580e-01, -7.50566721e-02,  3.45613480e-01,\n",
       "        9.98508930e-02, -1.27806723e-01,  1.50309458e-01,  1.47445500e-01,\n",
       "       -2.18655407e-01,  2.16275901e-01,  1.51288003e-01, -1.79667756e-01,\n",
       "        3.28646749e-01, -3.95458564e-02, -1.61604390e-01, -1.66342095e-01,\n",
       "        4.27057408e-02,  5.47738075e-01, -2.11499155e-01, -2.88481414e-01,\n",
       "        1.48779824e-02,  8.63390334e-04,  9.02139395e-02, -2.06250221e-01,\n",
       "        3.51726770e-01,  1.07848123e-02,  6.97113425e-02,  1.15963638e-01,\n",
       "       -1.53300881e-01,  2.12876145e-02,  5.10495119e-02,  2.02703640e-01,\n",
       "       -5.43372892e-03, -1.86824158e-01,  1.90662652e-01, -2.11743802e-01,\n",
       "        2.86564350e-01,  2.08971366e-01, -2.89304763e-01, -2.03481857e-02,\n",
       "        3.94561440e-01,  2.93006182e-01,  4.16626304e-01, -2.60198176e-01,\n",
       "       -1.79539591e-01,  1.98654145e-01,  8.96528736e-02,  3.37569058e-01,\n",
       "        9.65828598e-02,  2.60479927e-01, -2.61901379e-01, -9.24400464e-02,\n",
       "        3.08707845e-03,  1.78265497e-01, -2.70820558e-01, -2.24266335e-01,\n",
       "        2.07016587e-01, -1.31037980e-01,  7.68620074e-02,  7.90025340e-04,\n",
       "       -1.89994708e-01, -2.82383800e-01, -2.08901465e-01, -4.91521470e-02,\n",
       "       -8.43272060e-02,  1.82926789e-01,  3.88218284e-01,  1.95399895e-01,\n",
       "       -5.45533359e-01,  5.95503151e-01,  1.08843431e-01,  2.68812012e-02,\n",
       "       -1.03646882e-01, -8.97848457e-02,  6.19070567e-02, -2.41373125e-02,\n",
       "       -1.55334800e-01, -5.22878945e-01,  1.94789059e-02, -2.44774714e-01,\n",
       "        4.48390208e-02,  2.19686061e-01, -8.12795758e-02, -4.42700312e-02,\n",
       "       -1.29194319e-01,  3.71539205e-01,  4.26053889e-02,  1.50365919e-01,\n",
       "       -1.06485792e-01, -3.25452507e-01, -3.29276741e-01,  2.54020631e-01,\n",
       "       -1.04413673e-01, -3.12505364e-01, -5.63432276e-02, -2.62481153e-01,\n",
       "        3.81209284e-01, -3.36456805e-01,  2.27552623e-01,  4.08280864e-02,\n",
       "        2.04858243e-01,  1.28983021e-01,  7.07802400e-02, -2.42004693e-01,\n",
       "       -6.65890798e-02,  2.99389958e-01, -4.15875137e-01, -1.94615394e-01,\n",
       "       -2.07017432e-03, -8.41209218e-02, -1.24178134e-01, -7.27716181e-03,\n",
       "       -1.92899540e-01, -9.50756222e-02, -1.09613284e-01,  5.29353693e-02,\n",
       "       -8.88800919e-02, -1.15396269e-01,  2.83934385e-01,  1.33050382e-01,\n",
       "        6.76203072e-02,  1.26426890e-01, -3.49073089e-03, -1.23092391e-01,\n",
       "        9.08422247e-02, -1.99740395e-01, -8.51137266e-02,  2.20485687e-01,\n",
       "       -4.02816534e-02,  6.57603610e-03,  2.17157993e-02,  9.69271455e-03,\n",
       "       -5.86520024e-02, -2.15169743e-01, -1.41128898e-01, -1.32514372e-01,\n",
       "        1.85358375e-01, -1.64719224e-01,  8.47544298e-02, -1.89197622e-02,\n",
       "        7.08853528e-02,  2.57993221e-01, -1.28669173e-01,  7.08519667e-02,\n",
       "        6.20841756e-02,  2.38528959e-02, -3.30081999e-01,  1.20184444e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['信']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 6321 6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:64: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.dataSource = config.dataSource\n",
    "        self.stopWordSource = config.stopWordSource  \n",
    "        # 每条输入的序列处理为定长\n",
    "        self.sequenceLength = config.sequenceLength  \n",
    "        self.embeddingSize = config.embeddingSize\n",
    "        self.rate = config.rate\n",
    "        self.miniFreq=config.miniFreq\n",
    "        self.stopWordDict = {}\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding =None\n",
    "        self.n_symbols=0\n",
    "        self.wordToIndex = {}\n",
    "        self.indexToWord = {}\n",
    "        \n",
    "    def readData(self, filePath):\n",
    "        import random\n",
    "        with open(filePath) as fr:\n",
    "            data=fr.readlines()\n",
    "        random.shuffle(data)\n",
    "        with open(filePath,'w') as fw:\n",
    "            fw.write(''.join(data))\n",
    "        text=[]\n",
    "        label=[]\n",
    "        with open(filePath) as fr:\n",
    "            for line in file:\n",
    "                temp=line.strip().split('\\t')\n",
    "                text.append(temp[0])\n",
    "                label.append(temp[1])\n",
    "        print('data:',len(text),len(label))\n",
    "#         texts = [jieba.cut(document) for document in text]\n",
    "        texts = [list(document) for document in text]\n",
    "        return texts, label\n",
    "    \n",
    "    def readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    def getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        #中文\n",
    "        model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        wordEmbedding.append(np.zeros(self.embeddingSize))\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self.embeddingSize))\n",
    "        for word in words:\n",
    "            vector =model[word]\n",
    "            vocab.append(word)\n",
    "            wordEmbedding.append(vector)           \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        #去掉停用词\n",
    "#         subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        #统计词频，排序\n",
    "#         wordCount = Counter(subWords)\n",
    "        wordCount = Counter(allWords)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        #去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= self.miniFreq ]\n",
    "        #获取词列表和顺序对应的预训练权重矩阵\n",
    "        vocab, wordEmbedding = self.getWordEmbedding(words)\n",
    "        \n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self.wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self.indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        self.n_symbols = len(self.wordToIndex) + 1\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.wordToIndex, f)\n",
    "        with open(\"../wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.indexToWord, f)\n",
    "            \n",
    "    def reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "        每条评论 用index组成的定长数组来表示\n",
    "        \"\"\"\n",
    "        review=list(review)\n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "        return reviewVec\n",
    "\n",
    "    def genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self.reviewProcess(x[i], self.sequenceLength, self.wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            labels.append([y[i]])    \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        #trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        #evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "         \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        #读取停用词\n",
    "        self.readStopWord(self.stopWordSource)\n",
    "        #读取数据集\n",
    "        reviews, labels = self.readData(self.dataSource)\n",
    "        #分词、去停用词\n",
    "        #生成 词汇-索引 映射表和预训练权重矩阵，并保存\n",
    "        self.genVocabulary(reviews)\n",
    "        #初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self.genTrainEvalData(reviews, labels, self.rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (5056, 20)\n",
      "train label shape: (5056, 1)\n",
      "eval data shape: (1265, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 200)           236200    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 20, 200, 1)        0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1, 1, 768)         538368    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 4614      \n",
      "=================================================================\n",
      "Total params: 779,182\n",
      "Trainable params: 779,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def convolution(config):\n",
    "    sequence_length=config.sequenceLength\n",
    "    embedding_dimension=config.embeddingSize\n",
    "    \n",
    "    inn = Input(shape=(sequence_length, embedding_dimension, 1))\n",
    "    cnns = []\n",
    "    filter_sizes=config.filterSizes\n",
    "    for size in filter_sizes:\n",
    "        conv = Conv2D(filters=config.numFilters, kernel_size=(size, embedding_dimension),\n",
    "                            strides=1, padding='valid', activation='relu')(inn)\n",
    "        pool = MaxPool2D(pool_size=(sequence_length-size+1, 1), padding='valid')(conv)\n",
    "        cnns.append(pool)\n",
    "    outt =concatenate(cnns)\n",
    "    model = Model(inputs=inn, outputs=outt)\n",
    "    return model\n",
    "\n",
    "def cnn_mulfilter(n_symbols,embedding_weights,config):\n",
    "    model =Sequential([\n",
    "        Embedding(input_dim=n_symbols, output_dim=config.embeddingSize,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=config.sequenceLength, trainable=True),\n",
    "        Reshape((config.sequenceLength, config.embeddingSize, 1)),\n",
    "        convolution(config),\n",
    "        Flatten(),\n",
    "#         Dense(10, activation='relu',kernel_regularizer=regularizers.l2(config.l2RegLambda)),\n",
    "#         Dropout(config.dropoutKeepProb),\n",
    "        Dense(6, activation='softmax')])\n",
    "    \n",
    "#     model.compile(loss='SparseCategoricalCrossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                 loss=losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "model = cnn_mulfilter(n_symbols,wordEmbedding,config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/20\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 1.1108 - accuracy: 0.5678 - val_loss: 0.5969 - val_accuracy: 0.7777\n",
      "Epoch 2/20\n",
      "4044/4044 [==============================] - 4s 901us/sample - loss: 0.5054 - accuracy: 0.8200 - val_loss: 0.4123 - val_accuracy: 0.8587\n",
      "Epoch 3/20\n",
      "4044/4044 [==============================] - 3s 863us/sample - loss: 0.3492 - accuracy: 0.8848 - val_loss: 0.3780 - val_accuracy: 0.8715\n",
      "Epoch 4/20\n",
      "4044/4044 [==============================] - 4s 940us/sample - loss: 0.2635 - accuracy: 0.9169 - val_loss: 0.3761 - val_accuracy: 0.8696\n",
      "Epoch 5/20\n",
      "4044/4044 [==============================] - 4s 919us/sample - loss: 0.2065 - accuracy: 0.9325 - val_loss: 0.3795 - val_accuracy: 0.8706\n",
      "Epoch 6/20\n",
      "4044/4044 [==============================] - 4s 941us/sample - loss: 0.1625 - accuracy: 0.9530 - val_loss: 0.3863 - val_accuracy: 0.8735\n",
      "Epoch 7/20\n",
      "4044/4044 [==============================] - 4s 914us/sample - loss: 0.1267 - accuracy: 0.9669 - val_loss: 0.4022 - val_accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "4044/4044 [==============================] - 4s 952us/sample - loss: 0.1002 - accuracy: 0.9748 - val_loss: 0.4224 - val_accuracy: 0.8706\n",
      "Epoch 9/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0746 - accuracy: 0.9854 - val_loss: 0.4415 - val_accuracy: 0.8646\n",
      "Epoch 10/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0606 - accuracy: 0.9904 - val_loss: 0.4543 - val_accuracy: 0.8686\n",
      "Epoch 11/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0480 - accuracy: 0.9923 - val_loss: 0.4733 - val_accuracy: 0.8607\n",
      "Epoch 12/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0382 - accuracy: 0.9955 - val_loss: 0.4938 - val_accuracy: 0.8607\n",
      "Epoch 13/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0312 - accuracy: 0.9968 - val_loss: 0.5088 - val_accuracy: 0.8607\n",
      "Epoch 14/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0248 - accuracy: 0.9980 - val_loss: 0.5250 - val_accuracy: 0.8656\n",
      "Epoch 15/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0198 - accuracy: 0.9985 - val_loss: 0.5414 - val_accuracy: 0.8626\n",
      "Epoch 16/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0167 - accuracy: 0.9985 - val_loss: 0.5622 - val_accuracy: 0.8597\n",
      "Epoch 17/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.5666 - val_accuracy: 0.8617\n",
      "Epoch 18/20\n",
      "4044/4044 [==============================] - 4s 958us/sample - loss: 0.0110 - accuracy: 0.9995 - val_loss: 0.5801 - val_accuracy: 0.8607\n",
      "Epoch 19/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0093 - accuracy: 0.9995 - val_loss: 0.5911 - val_accuracy: 0.8636\n",
      "Epoch 20/20\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 0.8626\n",
      "1265/1265 [==============================] - 0s 183us/sample - loss: 0.6856 - accuracy: 0.8522\n",
      "test_loss: 0.685571, accuracy: 0.852174\n"
     ]
    }
   ],
   "source": [
    "x_train = data.trainReviews\n",
    "y_train = data.trainLabels\n",
    "x_eval = data.evalReviews\n",
    "y_eval = data.evalLabels\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_eval = to_categorical(y_eval)\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('./model/best_model/model_{epoch:02d}-{val_accuracy:.2f}.hdf5', save_best_only=True, save_weights_only=True)\n",
    "# history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "#验证\n",
    "scores = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "#保存模型\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./model/textCNN.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "model.save_weights('./model/textCNN.h5')\n",
    "\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BinaryCrossentropy',\n",
       " 'CategoricalCrossentropy',\n",
       " 'CategoricalHinge',\n",
       " 'CosineSimilarity',\n",
       " 'Hinge',\n",
       " 'Huber',\n",
       " 'KLD',\n",
       " 'KLDivergence',\n",
       " 'LogCosh',\n",
       " 'LogLoss',\n",
       " 'Loss',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " 'MeanAbsoluteError',\n",
       " 'MeanAbsolutePercentageError',\n",
       " 'MeanSquaredError',\n",
       " 'MeanSquaredLogarithmicError',\n",
       " 'Poisson',\n",
       " 'Reduction',\n",
       " 'SparseCategoricalCrossentropy',\n",
       " 'SquaredHinge',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_crossentropy',\n",
       " 'categorical_hinge',\n",
       " 'cosine_similarity',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'logcosh',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'serialize',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'squared_hinge']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [4.],\n",
       "       [3.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
