{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input,Conv2D,MaxPool2D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "import yaml\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    #数据集路径\n",
    "    dataSource = \"../single.txt\"\n",
    "    stopWordSource = \"../stopword.txt\"\n",
    "    #分词后保留大于等于最低词频的词\n",
    "    miniFreq=1\n",
    "    #统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n",
    "    sequenceLength = 30 \n",
    "    batchSize=64\n",
    "    epochs=50\n",
    "    numClasses = 6\n",
    "    #训练集的比例\n",
    "    rate = 0.8  \n",
    "    #生成嵌入词向量的维度\n",
    "    embeddingSize = 200\n",
    "    #卷积核数\n",
    "    numFilters = 128\n",
    "    #卷积核大小\n",
    "    filterSizes = [1,2,3,4,5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    #L2正则系数\n",
    "    l2RegLambda = 0.01\n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文语料\n",
    "#设置输出日志\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "file = open(\"../single.txt\") \n",
    "sentences=[]\n",
    "with open('../single.txt') as fr:\n",
    "    for line in fr.readlines():\n",
    "        temp=line.strip().split('\\t')\n",
    "#         sentences.append(jieba.lcut(temp[0]))\n",
    "        sentences.append(list(temp[0]))\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,size=config.embeddingSize,\n",
    "                     min_count=config.miniFreq,\n",
    "                     window=10,\n",
    "                     workers=multiprocessing.cpu_count(),sg=1,\n",
    "                     iter=20)\n",
    "model.save('../word2VecModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['我', '要', '了', '解', '不', '是', '单', '身', '哦', '那', '个', '呃', '回', '头', '再', '说', '好', '吗', '没', '有', '啊', '找', '到', '哪', '里', '呀', '就', '贴', '画', '你', '搞', '错', '吧', '行', '现', '在', '外', '面', '办', '事', '情', '寺', '庙', '拉', '出', '来', '朋', '友', '支', '付', '宝', '睡', '觉', '嗯', '用', '谢', '已', '经', '哈', '接', '电', '话', '这', '边', '哎', '投', '多', '少', '年', '前', '的', '大', '姐', '上', '班', '呢', '方', '便', '需', '他', '们', '完', '处', '理', '对', '钱', '想', '法', '段', '时', '间', '些', '先', '进', '什', '么', '但', '转', '女', '士', '意', '思', '退', '打', '哼', '资', '料', '哟', '听', '筒', '明', '白', '改', '忙', '咱', '活', '老', '婆', '试', '很', '久', '之', '发', '晚', '点', '比', '较', '开', '车', '也', '手', '机', '候', '给', '宅', '急', '三', '二', '毛', '线', '例', '小', '心', '会', '闹', '着', '玩', '太', '国', '知', '道', '同', '空', '正', '聊', '像', '陪', '又', '拿', '可', '以', '还', '最', '近', '建', '一', '常', '嘞', '天', '啥', '等', '致', '号', '然', '后', '怎', '善', '交', '象', '帮', '注', '销', '掉', '把', '账', '户', '离', '异', '谈', '早', '脑', '都', '人', '家', '刚', '住', '始', '放', '店', '云', '婚', '写', '靠', '弄', '麻', '烦', '问', '题', '下', '登', '录', '能', '联', '系', '去', '展', '珍', '爱', '网', '您', '流', '成', '啦', '生', '十', '姓', '梯', '六', '哇', '更', '换', '够', '疯', '哥', '样', '子', '抱', '歉', '喂', '讲', '跟', '笑', '露', '站', '本', '真', '见', '累', '差', '别', '册', '暂', '测', '过', '遍', '两', '考', '虑', '花', '缓', '特', '苗', '看', '儿', '待', '结', '撤', '查', '记', '沟', '通', '提', '示', '北', '京', '刷', '南', '门', '起', '相', '信', '拜', '慢', '超', '市', '从', '燕', '萝', '卜', '卖', '淘', '东', '西', '直', '呐', '诶', '耶', '坏', '兴', '趣', '逛', '街', '关', '民', '芬', '马', '密', '码', '输', '入', '挂', '填', '断', '呗', '求', '将', '新', '摄', '狗', '丽', '款', '挺', '变', '化', '今', '八', '分', '吵', '乖', '买', '才', '目', '红', '章', '取', '消', '五', '月', '份', '谁', '卸', '载', '男', '风', '吃', '饭', '返', '得', '核', '当', '咋', '况', '扣', '只', '嘛', '妈', '永', '远', '往', '神', '病', '房', '穿', '续', '赶', '快', '显', '字', '孩', '母', '亲', '而', '且', '庭', '因', '为', '俩', '员', '重', '庆', '删', '除', '丁', '勒', '内', '算', '收', '猫', '反', '书', '丹', '清', '楚', '和', '吓', '公', '司', '如', '果', '卡', '爸', '做', '每', '客', '稍', '怪', '乱', '观', '察', '请', '昨', '该', '死', '介', '绍', '妹', '七', '必', '摸', '高', '张', '嗳', '幺', '晕', '拖', '其', '代', '几', '饿', '碰', '或', '业', '务', '胡', '申', '应', '审', '感', '货', '九', '速', '水', '领', '脏', '休', '息', '懂', '良', '费', '搬', '次', '长', '口', '语', '银', '日', '期', '中', '山', '香', '旺', '它', '孙', '浩', '美', '虽', '扯', '坐', '俗', '裙', '星', '奇', '搜', '索', '借', '喽', '地', '浙', '江', '自', '己', '学', '习', '及', '状', '态', '剪', '平', '安', '推', '茫', '位', '另', '曾', '海', '金', '达', '板', '紧', '管', '光', '纤', '顾', '苏', '级', '蛮', '举', '报', '胃', '佳', '叫', '合', '照', '华', '夏', '敢', '鸡', '干', '订', '作', '曝', '适', '配', '委', '托', '咨', '询', '虫', '盘', '工', '望', '全', '被', '佛', '她', '千', '亩', '址', '龄', '广', '州', '深', '圳', '认', '购', '龙', '按', '攒', '舒', '服', '师', '原', '留', '限', '父', '怀', '孕', '连', '传', '闭', '块', '滚', '逼', '主', '左', '基', '质', '量', '肯', '定', '场', '整', '俞', '坤', '呵', '签', '决', '飞', '奥', '辈', '季', '随', '激', '所', '脆', '周', '末', '证', '追', '蓝', '微', '陈', '扔', '带', '似', '件', '准', '备', '室', '物', '龟', '熊', '杨', '欠', '昏', '洗', '省', '零', '万', '咯', '教', '翰', '甚', '至', '百', '扫', '描', '荐', '减', '肥', '库', '蓓', '蕾', '哆', '哩', '络', '满', '午', '优', '势', '昌', '划', '软', '效', '走', '部', '船', '总', '乐', '视', '标', '送', '废', '路', '复', '混', '寄', '忘', '确', '垃', '圾', '挣', '者', '于', '雅', '莞', '惯', '巧', '诉', '伞', '课', '名', '橙', '色', '动', '呼', '吸', '启', '式', '王', '恶', '加', '拼', '愿', '沙', '捞', '虎', '顺', '丰', '受', '益', '负', '责', '笨', '版', '型', '绑', '停', '炮', '品', '际', '途', '柜', '称', '杭', '宁', '波', '芝', '甜', '喊', '助', '半', '元', '世', '田', '答', '维', '修', '实', '岁', '呆', '鸟', '闻', '影', '院', '保', '险', '滴', '兰', '驼', '使', '慈', '籽', '调', '抽', '笔', '向', '台', '运', '怕', '获', '尽', '突', '假', '李', '瞎', '潇', '邱', '歌', '贝', '媳', '妇', '术', '恐', '读', '伤', '众', '孔', '牌', '骂', '温', '节', '未', '铁', '诺', '春', '贵', '武', '汉', '非', '夫', '声', '音', '倒', '无', '独', '酬', '体', '让', '与', '棍', '楼', '排', '醒', '利', '种', '止', '针', '议', '葫', '芦', '糊', '邓', '火', '炬', '产', '性', '额', '持', '魏', '摔', '骨', '折', '医', '丢', '选', '择', '并', '操', '包', '装', '侄', '鬼', '简', '汪', '告', '剩', '胆', '邮', '赞', '爆', '功', '强', '冻', '诞', '厦', '项', '误', '鞋', '雨', '军', '票', '气', '捍', '卫', '第', '详', '细', '黑', '钟', '识', '耽', '染', '厂', '穷', '黄', '瑶', '晓', '骑', '镇', '耍', '双', '盛', '惠', '尔', '售', '宜', '伴', '插', '终', '存', '侵', '犯', '瘢', '痕', '商', '床', '此', '访', '辟', '免', '控', '聪', '党', '参', '拨', '制', '舅', '携', '程', '肖', '噢', '辉', '帐', '均', '萨', '晨', '嘴', '背', '嫂', '精', '力', '博', '递', '首', '越', '喜', '欢', '浪', '频', '占', '页', '表', '娃', '底', '韩', '验', '吼', '阵', '汽', '积', '脉', '怂', '恿', '右', '阳', '承', '文', '涨', '伪', '造', '蔡', '伙', '食', '规', '倩', '钢', '木', '瓦', '唱', '罗', '预', '具', '补', '杠', '朗', '诵', '导', '扰', '设', '杂', '厅', '何', '闲', '吐', '婶', '睁', '印', '箱', '残', '疾', '类', '隋', '菲', '菜', '释', '徐', '誉', '格', '区', '置', '充', '幸', '福', '冯', '顶', '压', '汝', '静', '约', '啷', '颗', '森', '降', '祖', '宗', '亮', '夜', '却', '餐', '饮', '愉', '缴', '纳', '奖', '弯', '衡', '毁', '允', '许', '药', '估', '计', '套', '硬', '逊', '克', '汇', '喝', '疼', '堂', '萌', '圆', '政', '府', '姑', '娘', '须', '跑', '步', '束', '唉', '替', '享', '摘', '切', '巨', '继', '英', '绕', '牛', '疗', '培', '养', '凯', '旋', '革', '图', '片', '宏', '吴', '乡', '村', '立', '刻', '欧', '研', '帅', '壳', '校', '梦', '肴', '衫', '社', '林', '嘿', '赢', '尊', '严', '漏', '油', '迪', '聚', '盒', '棋', '著', '扎', '爷', '曦', '跪', '禁', '言', '警', '腰', '掰', '容', '易', '度', '郑', '评', '价', '甥', '材', '噻', '冷', '战', '播', '苹', '桌', '煮', '短', '余', '鲜', '币', '酒', '羊', '属', '胎', '彩', '据', '懒', '糖', '慧', '喔', '旁', '呦', '治', '素', '指', '揭', '救', '唐', '河', '扇', '挖', '纬', '吨', '拒', '绝', '念', '婷', '冠', '懵', '玉', '玺', '挽', '射', '繁', '律', '胜', '旅', '布', '凡', '埋', '怨', '剑', '姬', '迷', '黎', '烟', '草', '担', '曼', '迟', '沈', '恋', '熟', '条', '富', '邦', '唯', '尾', '晰', '尚', '卷', '屁', '模', '屋', '器', '难', '雄', '苑', '拆', '迁', '栾', '赵', '骤', '兵', '案', '技', '拥', '般', '阴', '钻', '石', '疙', '瘩', '赠', '搁', '梁', '粮', '阐', '述', '尝', '层', '眼', '睛', '失', '冒', '论', '撒', '缺', '初', '仓', '刘', '逸', '游', '戏', '肾', '键', '固', '城', '伟', '球', '招', '句', '毒', '舍', '翻', '衣', '湾', '训', '胖', '溜', '创', '框', '令', '覃', '振', '响', '困', '根', '局', '贸', '共', '昂', '诗', '寅', '芳', '故', '科', '钩', '叔', '颖', '弹', '恤', '棉', '港', '弟', '财', '征', '浴', '巾', '坑', '弱', '骗', '虚', '惹', '批', '奶', '劣', '列', '陆', '嗓', '绳', '唔', '值', '寻', '拦', '截', '租', '触', '冲', '凉', '浏', '览', '撂', '康'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.29857790e-02,  4.91257943e-02, -2.41246507e-01,  4.34472322e-01,\n",
       "        3.98935467e-01,  1.27987146e-01, -2.76224352e-02, -3.06991041e-01,\n",
       "        8.95354003e-02,  5.17200269e-02,  1.94119826e-01,  1.30522355e-01,\n",
       "       -1.27411127e-01, -2.04960108e-01, -2.97124535e-01, -3.92304081e-03,\n",
       "        3.44849825e-01, -2.29072377e-01, -4.17496860e-01, -1.85405642e-01,\n",
       "       -9.22740400e-02,  2.06548288e-01, -4.10946384e-02, -2.59862561e-02,\n",
       "        2.37759486e-01,  1.96844414e-01, -5.80747500e-02, -2.77664274e-01,\n",
       "       -5.41746952e-02,  1.81302086e-01,  5.53342630e-04,  5.28733552e-01,\n",
       "       -1.10973738e-01, -8.29479471e-02,  8.15340318e-03,  1.34053767e-01,\n",
       "        1.32584825e-01,  3.29553276e-01,  6.69638515e-02,  9.62608680e-02,\n",
       "        3.24829012e-01,  2.89054811e-01, -1.71548724e-02, -7.92213529e-02,\n",
       "        1.02995627e-01, -6.97705224e-02, -4.08988863e-01,  2.24829257e-01,\n",
       "       -1.40099123e-01,  8.06888193e-02,  3.97735894e-01, -1.14948623e-01,\n",
       "       -1.00040007e-02, -4.24990386e-01,  3.44610840e-01,  2.70537257e-01,\n",
       "        1.84764028e-01, -1.86902657e-01, -9.84200612e-02, -1.93499744e-01,\n",
       "       -2.52410606e-03, -4.17583793e-01,  8.50772709e-02,  1.49177447e-01,\n",
       "        2.54723996e-01, -6.66552456e-03, -3.45303982e-01, -1.30309939e-01,\n",
       "       -2.92111069e-01,  6.65851608e-02, -8.36704373e-02, -7.93486238e-02,\n",
       "       -1.30347610e-02,  1.15214579e-01, -3.71811450e-01,  5.74072897e-01,\n",
       "       -1.61356702e-01, -3.15586448e-01,  2.77095377e-01, -5.99655271e-01,\n",
       "       -3.63410190e-02, -2.79023737e-01,  4.01364475e-01,  2.89421193e-02,\n",
       "        3.69231473e-03,  2.18602836e-01,  8.00011680e-02,  2.44160280e-01,\n",
       "       -2.31362149e-01,  6.39610738e-02, -6.16802275e-01,  2.40858961e-02,\n",
       "        1.28878122e-02,  2.48732395e-03, -7.90715516e-02,  2.51951545e-01,\n",
       "        2.83489674e-01,  2.20081642e-01, -1.13118544e-01, -2.46694401e-01,\n",
       "        3.45457345e-02, -1.26106888e-01,  1.02252938e-01,  3.27306688e-01,\n",
       "        3.82578850e-01, -6.85721934e-02,  1.74457997e-01, -5.59453890e-02,\n",
       "        4.93568517e-02, -3.45022604e-02, -1.10096902e-01,  3.14509571e-02,\n",
       "        1.57839984e-01, -3.11541706e-01,  1.68760031e-01,  1.89426944e-01,\n",
       "        1.45873493e-02, -3.67886238e-02, -1.62498087e-01,  2.85963535e-01,\n",
       "       -1.77581161e-01, -1.34542391e-01, -1.66045666e-01, -3.02090161e-02,\n",
       "       -2.97060534e-02, -1.71923250e-01, -1.16812840e-01,  1.29838511e-01,\n",
       "       -5.20616055e-01, -3.62960100e-02, -3.61815095e-01,  6.92859218e-02,\n",
       "       -3.42105538e-01, -4.71664757e-01, -8.89653638e-02,  2.11535573e-01,\n",
       "        5.96673451e-02,  3.31508040e-01,  1.37194335e-01, -1.01328187e-01,\n",
       "       -1.40576288e-01,  1.21883735e-01,  1.74917564e-01,  3.87176514e-01,\n",
       "        1.07850917e-01,  3.90733704e-02,  3.42957854e-01, -2.86235332e-01,\n",
       "       -1.11152597e-01, -1.40041500e-01, -1.43042475e-01,  1.14756793e-01,\n",
       "        1.37035325e-01,  1.49071679e-01, -2.14090109e-01,  4.35712375e-02,\n",
       "       -3.02526742e-01,  7.77317062e-02,  2.53415108e-02,  2.05497950e-01,\n",
       "        6.01089969e-02, -2.69076854e-01, -3.60479439e-03,  3.19421351e-01,\n",
       "       -1.47831544e-01,  4.74720299e-02, -1.27656147e-01,  2.30736628e-01,\n",
       "        3.21572572e-01, -1.14416450e-01, -3.78586859e-01,  8.37885812e-02,\n",
       "       -2.95203745e-01,  3.60382438e-01, -7.54112974e-02, -2.63895653e-02,\n",
       "        6.64677471e-02,  3.16317588e-01,  4.28927876e-02,  2.20824420e-01,\n",
       "        7.13434257e-03,  1.43101644e-02,  1.11068739e-02, -2.17855066e-01,\n",
       "        1.82276770e-01,  1.11967623e-01, -2.24079385e-01, -3.22470993e-01,\n",
       "       -1.80949509e-01,  1.64741650e-01,  9.13722143e-02, -3.51669580e-01,\n",
       "        1.99350908e-01, -1.83011293e-01, -2.45387435e-01, -3.89320739e-02,\n",
       "       -4.00703341e-01, -8.37884769e-02,  6.15544468e-02, -2.04878822e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['信']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 19359 19359\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.dataSource = config.dataSource\n",
    "        self.stopWordSource = config.stopWordSource  \n",
    "        # 每条输入的序列处理为定长\n",
    "        self.sequenceLength = config.sequenceLength  \n",
    "        self.embeddingSize = config.embeddingSize\n",
    "        self.rate = config.rate\n",
    "        self.miniFreq=config.miniFreq\n",
    "        self.stopWordDict = {}\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding =None\n",
    "        self.n_symbols=0\n",
    "        self.wordToIndex = {}\n",
    "        self.indexToWord = {}\n",
    "        \n",
    "    def readData(self, filePath):\n",
    "        text=[]\n",
    "        label=[]\n",
    "        with open(filePath) as fr:\n",
    "            for line in file:\n",
    "                temp=line.strip().split('\\t')\n",
    "                text.append(temp[0])\n",
    "                label.append(temp[1])\n",
    "        print('data:',len(text),len(label))\n",
    "#         texts = [jieba.cut(document) for document in text]\n",
    "        texts = [list(document) for document in text]\n",
    "        return texts, label\n",
    "    \n",
    "    def readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    def getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        #中文\n",
    "        model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        wordEmbedding.append(np.zeros(self.embeddingSize))\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self.embeddingSize))\n",
    "        for word in words:\n",
    "            vector =model[word]\n",
    "            vocab.append(word)\n",
    "            wordEmbedding.append(vector)           \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        #去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        #统计词频，排序\n",
    "        wordCount = Counter(subWords)  \n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        #去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= self.miniFreq ]\n",
    "        #获取词列表和顺序对应的预训练权重矩阵\n",
    "        vocab, wordEmbedding = self.getWordEmbedding(words)\n",
    "        \n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self.wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self.indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        self.n_symbols = len(self.wordToIndex) + 1\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.wordToIndex, f)\n",
    "        with open(\"../wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.indexToWord, f)\n",
    "            \n",
    "    def reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "        每条评论 用index组成的定长数组来表示\n",
    "        \"\"\"\n",
    "        review=list(review)\n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "        return reviewVec\n",
    "\n",
    "    def genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self.reviewProcess(x[i], self.sequenceLength, self.wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            labels.append([y[i]])    \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        #trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        #evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "         \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        #读取停用词\n",
    "        self.readStopWord(self.stopWordSource)\n",
    "        #读取数据集\n",
    "        reviews, labels = self.readData(self.dataSource)\n",
    "        #分词、去停用词\n",
    "        #生成 词汇-索引 映射表和预训练权重矩阵，并保存\n",
    "        self.genVocabulary(reviews)\n",
    "        #初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self.genTrainEvalData(reviews, labels, self.rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (15487, 30)\n",
      "train label shape: (15487, 1)\n",
      "eval data shape: (3872, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 200)           208000    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 30, 200, 1)        0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1, 1, 640)         384640    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 3846      \n",
      "=================================================================\n",
      "Total params: 596,486\n",
      "Trainable params: 596,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def convolution(config):\n",
    "    sequence_length=config.sequenceLength\n",
    "    embedding_dimension=config.embeddingSize\n",
    "    \n",
    "    inn = Input(shape=(sequence_length, embedding_dimension, 1))\n",
    "    cnns = []\n",
    "    filter_sizes=config.filterSizes\n",
    "    for size in filter_sizes:\n",
    "        conv = Conv2D(filters=config.numFilters, kernel_size=(size, embedding_dimension),\n",
    "                            strides=1, padding='valid', activation='relu')(inn)\n",
    "        pool = MaxPool2D(pool_size=(sequence_length-size+1, 1), padding='valid')(conv)\n",
    "        cnns.append(pool)\n",
    "    outt =concatenate(cnns)\n",
    "    model = Model(inputs=inn, outputs=outt)\n",
    "    return model\n",
    "\n",
    "def cnn_mulfilter(n_symbols,embedding_weights,config):\n",
    "    model =Sequential([\n",
    "        Embedding(input_dim=n_symbols, output_dim=config.embeddingSize,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=config.sequenceLength, trainable=True),\n",
    "        Reshape((config.sequenceLength, config.embeddingSize, 1)),\n",
    "        convolution(config),\n",
    "        Flatten(),\n",
    "#         Dense(10, activation='relu',kernel_regularizer=regularizers.l2(config.l2RegLambda)),\n",
    "#         Dropout(config.dropoutKeepProb),\n",
    "        Dense(6, activation='softmax')])\n",
    "    \n",
    "#     model.compile(loss='SparseCategoricalCrossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                 loss=losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "model = cnn_mulfilter(n_symbols,wordEmbedding,config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12389 samples, validate on 3098 samples\n",
      "Epoch 1/50\n",
      "12389/12389 [==============================] - 7s 556us/sample - loss: 0.9071 - accuracy: 0.6599 - val_loss: 0.7610 - val_accuracy: 0.7198\n",
      "Epoch 2/50\n",
      "12389/12389 [==============================] - 6s 516us/sample - loss: 0.7006 - accuracy: 0.7419 - val_loss: 0.7456 - val_accuracy: 0.7256\n",
      "Epoch 3/50\n",
      "12389/12389 [==============================] - 6s 518us/sample - loss: 0.6572 - accuracy: 0.7566 - val_loss: 0.7394 - val_accuracy: 0.7331\n",
      "Epoch 4/50\n",
      "12389/12389 [==============================] - 6s 524us/sample - loss: 0.6255 - accuracy: 0.7664 - val_loss: 0.7392 - val_accuracy: 0.7292\n",
      "Epoch 5/50\n",
      "12389/12389 [==============================] - 7s 526us/sample - loss: 0.6020 - accuracy: 0.7730 - val_loss: 0.7501 - val_accuracy: 0.7243\n",
      "Epoch 6/50\n",
      "12389/12389 [==============================] - 7s 526us/sample - loss: 0.5813 - accuracy: 0.7788 - val_loss: 0.7397 - val_accuracy: 0.7318\n",
      "Epoch 7/50\n",
      "12389/12389 [==============================] - 6s 506us/sample - loss: 0.5626 - accuracy: 0.7838 - val_loss: 0.7629 - val_accuracy: 0.7295\n",
      "Epoch 8/50\n",
      "12389/12389 [==============================] - 6s 514us/sample - loss: 0.5496 - accuracy: 0.7882 - val_loss: 0.7822 - val_accuracy: 0.7276\n",
      "Epoch 9/50\n",
      "12389/12389 [==============================] - 6s 513us/sample - loss: 0.5356 - accuracy: 0.7948 - val_loss: 0.7645 - val_accuracy: 0.7279\n",
      "Epoch 10/50\n",
      "12389/12389 [==============================] - 7s 529us/sample - loss: 0.5195 - accuracy: 0.7989 - val_loss: 0.7758 - val_accuracy: 0.7327\n",
      "Epoch 11/50\n",
      "12389/12389 [==============================] - 7s 542us/sample - loss: 0.5124 - accuracy: 0.8010 - val_loss: 0.7981 - val_accuracy: 0.7218\n",
      "Epoch 12/50\n",
      "12389/12389 [==============================] - 7s 544us/sample - loss: 0.5041 - accuracy: 0.8049 - val_loss: 0.7859 - val_accuracy: 0.7321\n",
      "Epoch 13/50\n",
      "12389/12389 [==============================] - 7s 542us/sample - loss: 0.4940 - accuracy: 0.8058 - val_loss: 0.7907 - val_accuracy: 0.7292\n",
      "Epoch 14/50\n",
      "12389/12389 [==============================] - 7s 541us/sample - loss: 0.4873 - accuracy: 0.8083 - val_loss: 0.8036 - val_accuracy: 0.7292\n",
      "Epoch 15/50\n",
      "12389/12389 [==============================] - 7s 541us/sample - loss: 0.4857 - accuracy: 0.8119 - val_loss: 0.8124 - val_accuracy: 0.7260\n",
      "Epoch 16/50\n",
      "12389/12389 [==============================] - 7s 536us/sample - loss: 0.4792 - accuracy: 0.8118 - val_loss: 0.8173 - val_accuracy: 0.7272\n",
      "Epoch 17/50\n",
      "12389/12389 [==============================] - 7s 539us/sample - loss: 0.4757 - accuracy: 0.8139 - val_loss: 0.8278 - val_accuracy: 0.7208\n",
      "Epoch 18/50\n",
      "12389/12389 [==============================] - 7s 535us/sample - loss: 0.4679 - accuracy: 0.8148 - val_loss: 0.8259 - val_accuracy: 0.7218\n",
      "Epoch 19/50\n",
      "12389/12389 [==============================] - 7s 535us/sample - loss: 0.4658 - accuracy: 0.8154 - val_loss: 0.8413 - val_accuracy: 0.7205\n",
      "Epoch 20/50\n",
      "12389/12389 [==============================] - 7s 554us/sample - loss: 0.4595 - accuracy: 0.8155 - val_loss: 0.8573 - val_accuracy: 0.7201\n",
      "Epoch 21/50\n",
      "12389/12389 [==============================] - 7s 555us/sample - loss: 0.4613 - accuracy: 0.8172 - val_loss: 0.8552 - val_accuracy: 0.7205\n",
      "Epoch 22/50\n",
      "12389/12389 [==============================] - 7s 541us/sample - loss: 0.4563 - accuracy: 0.8172 - val_loss: 0.8558 - val_accuracy: 0.7221\n",
      "Epoch 23/50\n",
      "12389/12389 [==============================] - 7s 539us/sample - loss: 0.4565 - accuracy: 0.8193 - val_loss: 0.8564 - val_accuracy: 0.7237\n",
      "Epoch 24/50\n",
      "12389/12389 [==============================] - 7s 548us/sample - loss: 0.4507 - accuracy: 0.8210 - val_loss: 0.8748 - val_accuracy: 0.7198\n",
      "Epoch 25/50\n",
      "12389/12389 [==============================] - 7s 545us/sample - loss: 0.4527 - accuracy: 0.8181 - val_loss: 0.8776 - val_accuracy: 0.7153\n",
      "Epoch 26/50\n",
      "12389/12389 [==============================] - 7s 544us/sample - loss: 0.4494 - accuracy: 0.8197 - val_loss: 0.8767 - val_accuracy: 0.7169\n",
      "Epoch 27/50\n",
      "12389/12389 [==============================] - 7s 544us/sample - loss: 0.4460 - accuracy: 0.8193 - val_loss: 0.8841 - val_accuracy: 0.7169\n",
      "Epoch 28/50\n",
      "12389/12389 [==============================] - 7s 547us/sample - loss: 0.4431 - accuracy: 0.8218 - val_loss: 0.8778 - val_accuracy: 0.7218\n",
      "Epoch 29/50\n",
      "12389/12389 [==============================] - 7s 546us/sample - loss: 0.4449 - accuracy: 0.8202 - val_loss: 0.8836 - val_accuracy: 0.7124\n",
      "Epoch 30/50\n",
      "12389/12389 [==============================] - 7s 549us/sample - loss: 0.4417 - accuracy: 0.8239 - val_loss: 0.8915 - val_accuracy: 0.7172\n",
      "Epoch 31/50\n",
      "12389/12389 [==============================] - 7s 547us/sample - loss: 0.4429 - accuracy: 0.8216 - val_loss: 0.9060 - val_accuracy: 0.7172\n",
      "Epoch 32/50\n",
      "12389/12389 [==============================] - 7s 552us/sample - loss: 0.4401 - accuracy: 0.8249 - val_loss: 0.9134 - val_accuracy: 0.7218\n",
      "Epoch 33/50\n",
      "12389/12389 [==============================] - 7s 551us/sample - loss: 0.4382 - accuracy: 0.8238 - val_loss: 0.9055 - val_accuracy: 0.7147\n",
      "Epoch 34/50\n",
      "12389/12389 [==============================] - 7s 551us/sample - loss: 0.4368 - accuracy: 0.8244 - val_loss: 0.9096 - val_accuracy: 0.7224\n",
      "Epoch 35/50\n",
      "12389/12389 [==============================] - 7s 553us/sample - loss: 0.4383 - accuracy: 0.8248 - val_loss: 0.9259 - val_accuracy: 0.7221\n",
      "Epoch 36/50\n",
      "12389/12389 [==============================] - 7s 548us/sample - loss: 0.4368 - accuracy: 0.8234 - val_loss: 0.9432 - val_accuracy: 0.7140\n",
      "Epoch 37/50\n",
      "12389/12389 [==============================] - 7s 546us/sample - loss: 0.4335 - accuracy: 0.8240 - val_loss: 0.9374 - val_accuracy: 0.7153\n",
      "Epoch 38/50\n",
      "12389/12389 [==============================] - 7s 550us/sample - loss: 0.4351 - accuracy: 0.8249 - val_loss: 0.9545 - val_accuracy: 0.7053\n",
      "Epoch 39/50\n",
      "12389/12389 [==============================] - 7s 546us/sample - loss: 0.4362 - accuracy: 0.8213 - val_loss: 0.9494 - val_accuracy: 0.7156\n",
      "Epoch 40/50\n",
      "12389/12389 [==============================] - 7s 547us/sample - loss: 0.4321 - accuracy: 0.8237 - val_loss: 0.9298 - val_accuracy: 0.7218\n",
      "Epoch 41/50\n",
      "12389/12389 [==============================] - 7s 548us/sample - loss: 0.4305 - accuracy: 0.8243 - val_loss: 0.9726 - val_accuracy: 0.7159\n",
      "Epoch 42/50\n",
      "12389/12389 [==============================] - 7s 546us/sample - loss: 0.4328 - accuracy: 0.8262 - val_loss: 0.9483 - val_accuracy: 0.7153\n",
      "Epoch 43/50\n",
      "12389/12389 [==============================] - 7s 560us/sample - loss: 0.4295 - accuracy: 0.8254 - val_loss: 0.9602 - val_accuracy: 0.7201\n",
      "Epoch 44/50\n",
      "12389/12389 [==============================] - 7s 566us/sample - loss: 0.4286 - accuracy: 0.8279 - val_loss: 0.9557 - val_accuracy: 0.7105\n",
      "Epoch 45/50\n",
      "12389/12389 [==============================] - 7s 554us/sample - loss: 0.4318 - accuracy: 0.8250 - val_loss: 0.9607 - val_accuracy: 0.7137\n",
      "Epoch 46/50\n",
      "12389/12389 [==============================] - 7s 601us/sample - loss: 0.4302 - accuracy: 0.8244 - val_loss: 0.9664 - val_accuracy: 0.7143\n",
      "Epoch 47/50\n",
      "12389/12389 [==============================] - 8s 608us/sample - loss: 0.4299 - accuracy: 0.8256 - val_loss: 0.9628 - val_accuracy: 0.7101\n",
      "Epoch 48/50\n",
      "12389/12389 [==============================] - 8s 630us/sample - loss: 0.4279 - accuracy: 0.8270 - val_loss: 0.9479 - val_accuracy: 0.7127\n",
      "Epoch 49/50\n",
      "12389/12389 [==============================] - 8s 625us/sample - loss: 0.4274 - accuracy: 0.8261 - val_loss: 0.9687 - val_accuracy: 0.7088\n",
      "Epoch 50/50\n",
      "12389/12389 [==============================] - 8s 631us/sample - loss: 0.4275 - accuracy: 0.8282 - val_loss: 0.9688 - val_accuracy: 0.7095\n",
      "3872/3872 [==============================] - 0s 106us/sample - loss: 0.9282 - accuracy: 0.7270\n",
      "test_loss: 0.928233, accuracy: 0.727014\n"
     ]
    }
   ],
   "source": [
    "x_train = data.trainReviews\n",
    "y_train = data.trainLabels\n",
    "x_eval = data.evalReviews\n",
    "y_eval = data.evalLabels\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_eval = to_categorical(y_eval)\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('./model/best_model/model_{epoch:02d}-{val_accuracy:.2f}.hdf5', save_best_only=True, save_weights_only=True)\n",
    "# history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "#验证\n",
    "scores = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "#保存模型\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./model/textCNN.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "model.save_weights('./model/textCNN.h5')\n",
    "\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BinaryCrossentropy',\n",
       " 'CategoricalCrossentropy',\n",
       " 'CategoricalHinge',\n",
       " 'CosineSimilarity',\n",
       " 'Hinge',\n",
       " 'Huber',\n",
       " 'KLD',\n",
       " 'KLDivergence',\n",
       " 'LogCosh',\n",
       " 'LogLoss',\n",
       " 'Loss',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " 'MeanAbsoluteError',\n",
       " 'MeanAbsolutePercentageError',\n",
       " 'MeanSquaredError',\n",
       " 'MeanSquaredLogarithmicError',\n",
       " 'Poisson',\n",
       " 'Reduction',\n",
       " 'SparseCategoricalCrossentropy',\n",
       " 'SquaredHinge',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_crossentropy',\n",
       " 'categorical_hinge',\n",
       " 'cosine_similarity',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'logcosh',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'serialize',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'squared_hinge']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [5.],\n",
       "       [4.],\n",
       "       ...,\n",
       "       [5.],\n",
       "       [2.],\n",
       "       [2.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
