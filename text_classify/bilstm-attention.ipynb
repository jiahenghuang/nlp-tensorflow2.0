{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import BatchNormalization,Layer,Input,Conv2D,MaxPool2D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape,LSTM\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import multiprocessing\n",
    "import yaml\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    #数据集路径\n",
    "    dataSource = \"../marriage.txt\"\n",
    "    stopWordSource = \"../stopword.txt\"\n",
    "    #分词后保留大于等于最低词频的词\n",
    "    miniFreq=1\n",
    "    #统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n",
    "    sequenceLength = 30 \n",
    "    batchSize=128\n",
    "    epochs=100\n",
    "    numClasses = 5\n",
    "    #训练集的比例\n",
    "    rate = 0.8  \n",
    "    #生成嵌入词向量的维度\n",
    "    embeddingSize = 200\n",
    "    #卷积核数\n",
    "    numFilters = 128\n",
    "    #卷积核大小\n",
    "    filterSizes = [1,2,3,4,5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    #L2正则系数\n",
    "    l2RegLambda = 0.01\n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文语料\n",
    "#设置输出日志\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "file = open(\"../marriage.txt\") \n",
    "sentences=[]\n",
    "with open('../marriage.txt') as fr:\n",
    "    for line in fr.readlines():\n",
    "        temp=line.strip().split('\\t')\n",
    "#         sentences.append(jieba.lcut(temp[0]))\n",
    "        sentences.append(list(temp[0]))\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,size=config.embeddingSize,\n",
    "                     min_count=config.miniFreq,\n",
    "                     window=10,\n",
    "                     workers=multiprocessing.cpu_count(),sg=1,\n",
    "                     iter=20)\n",
    "model.save('../word2VecModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['嗯', '有', '啊', '反', '了', '看', '没', '那', '太', '好', '几', '年', '零', '四', '是', '的', '我', '现', '在', '边', '上', '客', '户', '哎', '你', '稍', '等', '一', '下', '先', '过', '挂', '掉', '这', '还', '办', '最', '久', '都', '两', '班', '可', '能', '次', '再', '完', '跟', '他', '见', '多', '认', '证', '就', '问', '题', '但', '三', '报', '考', '交', '离', '婚', '拿', '事', '时', '候', '直', '接', '今', '天', '晚', '早', '走', '对', '吧', '大', '米', '然', '后', '弄', '回', '来', '说', '让', '钱', '吗', '号', '厅', '七', '呀', '听', '很', '吵', '份', '整', '个', '拜', '哇', '倒', '投', '诉', '不', '单', '身', '陈', '倩', '豆', '瓣', '带', '放', '啦', '给', '留', '线', '人', '方', '便', '答', '哈', '丢', '八', '前', '哦', '换', '正', '版', '呢', '半', '怪', '家', '里', '面', '出', '全', '呃', '妹', '找', '司', '法', '着', '急', '电', '话', '行', '选', '咋', '肯', '定', '改', '搬', '总', '共', '念', '奇', '意', '思', '已', '经', '退', '进', '确', '到', '们', '协', '议', '书', '章', '明', '点', '每', '坐', '车', '把', '错', '包', '括', '强', '调', '喂', '去', '也', '干', '啥', '帮', '吓', '跳', '常', '声', '开', '相', '爱', '午', '打', '加', '油', '会', '玩', '咱', '聊', '二', '十', '目', '些', '讲', '想', '旅', '游', '九', '猫', '跑', '姐', '成', '满', '网', '怕', '种', '工', '作', '卡', '坏', '老', '外', '嘛', '官', '什', '么', '院', '解', '只', '裁', '决', '诶', '哟', '谢', '您', '记', '要', '提', '当', '关', '站', '结', '束', '以', '难', '铺', '五', '农', '水', '平', '礼', '因', '为', '间', '志', '友', '别', '北', '京', '搞', '李', '峰', '镇', '唉', '牌', '道', '知', '品', '真', '呗', '样', '哪', '珍', '姥', '手', '月', '卫', '辉', '之', '发', '谁', '派', '暂', '用', '长', '案', '照', '玛', '像', '注', '册', '小', '阵', '算', '所', '示', '检', '体', '谅', '饿', '抢', '怎', '刚', '刘', '文', '必', '春', '节', '六', '判', '摆', '骗', '室', '件', '妈', '铁', '岭', '沈', '阳', '东', '西', '邮', '箱', '处', '吃', '饭', '得', '婆', '本', '登', '蚊', '子', '部', '分', '赵', '导', '爸', '居', '幺', '噢', '优', '雅', '应', '蚂', '蚁', '嫂', '钟', '嘞', '淘', '宝', '薄', '情', '传', '票', '空', '供', '签', '才', '卸', '载', '力', '量', '起', '讼', '仔', '细', '更', '深', '录', '懂', '娃', '服', '务', '果', '浩', '写', '概', '临', '汾', '触', '少', '该', '底', '随', '往', '员', '麻', '烦', '资', '料', '销', '快', '递', '同', '凌', '晨', '地', '朋', '场', '售', '张', '白', '纸', '红', '清', '楚', '枣', '待', '拍', '鸡', '呵', '期', '哼', '安', '排', '亲', '觉', '需', '语', 'w', 'o', 'r', 'd', '根', '邻', '向', '信', '新', '娘', '差', '公', '儿', '伴', '战', '播', '星', '万', '另', '支', '持', '于', '孩', '理', '杜', '位', '律', '效', '核', '实', '昨', '中', '名', '申', '虑', '自', '己', '区', '县', '联', '系', '例', '迁', '移', '转', '舞', '蹈', '马', '除', 's', '企', '业', '账', '连', '革', '翻', '近', '男', '挺', '轻', '如', '补', '字', '贵', '州', '请', '害', '冯', '燕', '花', '朵', '填', '鞋', '告', '烊', '比', '惠', '香', '港', '寄', '具', '益', '鸿', '乐', '抱', '它', '叫', '影', '响', '初', '江', '苏', '域', '英', '绑', '瞬', '洪', '城', '并', '姻', '皆', '欢', '喜', '南', '台', '湾', '段', '疯', '狂', '码', '通', 'v', 'i', 'p', '牙', '医', '周', '或', '者', '又', '女', '生', '撤', '头', '涨', '牛', '舒', '辛', '苦', '扰', '准', '备', '银', '机', '主', '动', '活', '兜', '国', '晓', '使', '斗', '罗', '陆', '楼', '验', '草', '阶', '杨', '凉', '希', '望', '广', '穿', '从', '压', '观', '众', '政', '升', '级', '摄', '合', '代', '阿', '装', '睡', '梅', '师', '查', '而', '视', '频', '遭', '遇', '淡', '顺', '骏', '纯', '盘', '岁', '基', '建', '忙', '试', '患', '聪', '做', '魔', '高', '担', '心', '呐', '棒', '千', '重', '妞', '灵', '魂', '晕', '般', '迷', '汕', '感', '阴', '风', '无', '锋', 'k', '操', '领', '翰', '未', '菜', '石', '庄', '嗳', '困', '碰', '壮', '局', '逃', '避', '路', '指', '变', '日', '卖', '复', '杂', '紧', '取', '消', '废', '征', '含', '蛋', '受', '限', '毛', '唱', '学', '历', '遗', '忘', '慢', '咯', '材', '值', '据', '神', '病', '策', '丧', '偶', '续', '缘', '尝', '增', '弹', '撑', '订', '百', '元', '贷', '士', '斥', '论', '骂', '闷', '火', '庭', '首', '纠', '纷', '黄', '混', '尺', '寸', '洲', '美', '和', '芦', '荟', '胶', '霉', '货', '按', '层', '简', '靠', '板', '伤', '庆', '片', '曾', '死', '估', '计', '滚', '送', '罢', '笑', '洗', '澡', '其', '蛮', '歉', '击', '介', '绍', '碌', '民', '丰', '利', '既', '口', '鸭', '势', '股', '买', '逗', '傍', '她', '善', '表', '歌', '修', '房', '盐', '炒', '顾', '喽', '抓', '爆', '态', '度', '适', '且', '内', '杭', '梨', '青', '山', '4', '0', '丽', '左', '右', '印', '设', '置', '兆', '类', '拨', '款', '金', '额', '借', '扣', '任', '娜', '扔', '须', '吕', '旁', '赔', '流', '犯', '愤', '范', '被', '存', '乱', '呦', '异', '球', '丶', '求', '赶', '键', '断', '冻', '骑', '鬼', '秒', '研', '究', '纪', '显', '润', '貌', '似', '宁', '妥', '微', '特', '门', '铃', '收', '令', '圈', '教', '推', '读', '脚', '鸣', '智', '桂', '梦', '商', '亮', '付', '详', '攻', '闭', '失', '占', '保', '佑', '拖', '况', '横', '斩', '队', '耶', '馈', '费', '噻', '术', '扮', '姚', '甲', '逼', '汉', '易', '败', '套', '举', '档', '拌', '溜', '冰', '贴', '膜', '王', '拥', '泵', '厂', '象', '坤', '伯', '杰', '俊', '际', '气', '挣', '参', '假', '删', '醒', '雁', '音', '状', '毁', '软', '赚', '器', '浪', '招', '圳', '住', '挖', '掘', '称', '惨', '邓', '块', '毕', '竟', '咽', '唤', '欠', '戏', '呼', '华', '泰', '遍', '闻', '控', '积', '散', '均', '替', '贾', '玲', '笔', '贩', '欧', '荐', '权', '课', '番', '偷', '购', '程', '卦', 'a', ' ', '巴', '徐', '尽', '越', '够', '浓', '黎', '宏', '振', '怀', '疑', '科', '评', '哥', '维', '羞', '批', '戚', '双', '秋', '柿', '汤', '狗', '胖', '输', '化', '绩', '端', '入', '丁', '伟', '灾', '沙', '烧', '拔', '谓', '绿', '静', '盖', '袋', '府', '炮', '嫁', '嚎', '咨', '询', '团', '密', '栋', '钥', '匙', '嘟', '努', '原', '质', '启', '吞', '噬', '云', '焊', '访', '店', '康', '海', '扫', '释', '审', '价', '粘', '谈', '致', '较', '糖', '校', '宽', '卷', '列', '嗨', '艺', '耽', '搁', '第', '标', '林', '朝', '市', 'q', '独', '立', '由', '樱', '童', '杠', '捎', '谎', '言', '旧', '达', '湿', '探', '讨', '震', '迟', '速', '沟', '性', '虽', '闺', '夏', '远', '与', '布', '捺', '始', '夜', '兴', '索', '脸', '嘲', '赖', '及', '匹', '配', '雯', '座', '灯', '属', '监', '猜', '统', '厉', '锁', '型', '舔', '尾', '隐', '私', '误', '荤', '巾', '何', '浏', '览', '颜', '药', '映', '约', '骚', '渠', '拾', '践', '奢', '侈', '良', '绝', '各', '稿', '架', '圣', '剑', '妆', '划', '雨', '奖', '励', '险', '式', '透', '露', '姓', '析', '愿', '孙', '刷', '涂', '黑', '休', '息', '创', '搜', '咸', '呆', '婉', '管', '条', '闲', '添', '厕', '钻', '促', '光', '纤', 'm', '追', '余', '郁', '伙', '链', '助', '预', '父', '尔', '运', '刻', '侍', '形', '乳', '眼', '功', '累', '凭', '测', '傻', '脑', '扯', '糟', '充', '昌', '址', '诞', '烟', '物', '帐', '省', '豪', '颁', '诈', '专', '框', '昏', '僭', 'y', '齐', '凤', '凰', '瓦', '温', '瑞', '昆', '图', '甘', '肃', '劝', '肛', '扎', '拉', '拼', '世', '湖', '故'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 6321 6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.dataSource = config.dataSource\n",
    "        self.stopWordSource = config.stopWordSource  \n",
    "        # 每条输入的序列处理为定长\n",
    "        self.sequenceLength = config.sequenceLength  \n",
    "        self.embeddingSize = config.embeddingSize\n",
    "        self.rate = config.rate\n",
    "        self.miniFreq=config.miniFreq\n",
    "        self.stopWordDict = {}\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding =None\n",
    "        self.n_symbols=0\n",
    "        self.wordToIndex = {}\n",
    "        self.indexToWord = {}\n",
    "        \n",
    "    def readData(self, filePath):\n",
    "        text=[]\n",
    "        label=[]\n",
    "        with open(filePath) as fr:\n",
    "            for line in file:\n",
    "                temp=line.strip().split('\\t')\n",
    "                text.append(temp[0])\n",
    "                label.append(temp[1])\n",
    "        print('data:',len(text),len(label))\n",
    "#         texts = [jieba.cut(document) for document in text]\n",
    "        texts = [list(document) for document in text]\n",
    "        return texts, label\n",
    "    \n",
    "    def readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    def getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        #中文\n",
    "        model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        wordEmbedding.append(np.zeros(self.embeddingSize))\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self.embeddingSize))\n",
    "        for word in words:\n",
    "            vector =model[word]\n",
    "            vocab.append(word)\n",
    "            wordEmbedding.append(vector)           \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        #去掉停用词\n",
    "#         subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        #统计词频，排序\n",
    "#         wordCount = Counter(subWords)\n",
    "        wordCount = Counter(allWords)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        #去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= self.miniFreq ]\n",
    "        #获取词列表和顺序对应的预训练权重矩阵\n",
    "        vocab, wordEmbedding = self.getWordEmbedding(words)\n",
    "        \n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self.wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self.indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        self.n_symbols = len(self.wordToIndex) + 1\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.wordToIndex, f)\n",
    "        with open(\"../wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.indexToWord, f)\n",
    "            \n",
    "    def reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "        每条评论 用index组成的定长数组来表示\n",
    "        \"\"\"\n",
    "        review=list(review)\n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "        return reviewVec\n",
    "\n",
    "    def genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self.reviewProcess(x[i], self.sequenceLength, self.wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            labels.append([y[i]])    \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        #trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        #evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "         \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        #读取停用词\n",
    "        self.readStopWord(self.stopWordSource)\n",
    "        #读取数据集\n",
    "        reviews, labels = self.readData(self.dataSource)\n",
    "        #分词、去停用词\n",
    "        #生成 词汇-索引 映射表和预训练权重矩阵，并保存\n",
    "        self.genVocabulary(reviews)\n",
    "        #初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self.genTrainEvalData(reviews, labels, self.rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Python/3.7/site-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 200)           236200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 30, 400)           641600    \n",
      "_________________________________________________________________\n",
      "attention_layer (AttentionLa (None, 400)               930       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 2406      \n",
      "=================================================================\n",
      "Total params: 882,736\n",
      "Trainable params: 881,936\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = backend.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = backend.softmax(backend.tanh(backend.dot(x, self.W) + self.b))\n",
    "        outputs = backend.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = backend.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "\n",
    "\n",
    "def train_lstm(n_symbols,embedding_weights,config):\n",
    "    \n",
    "    model =Sequential([\n",
    "        Embedding(input_dim=n_symbols, output_dim=config.embeddingSize,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=config.sequenceLength),\n",
    "        \n",
    "    #LSTM层\n",
    "    Dropout(config.dropoutKeepProb),\n",
    "    Bidirectional(LSTM(200,activation='tanh', dropout=0.5, recurrent_dropout=0.5,return_sequences=True), merge_mode='concat'),\n",
    "    AttentionLayer(),\n",
    "    Dropout(config.dropoutKeepProb),\n",
    "    BatchNormalization(),\n",
    "    Dense(6, activation='softmax')])  \n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                 loss=losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "model = train_lstm(n_symbols,wordEmbedding,config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 1.6112 - accuracy: 0.3776 - val_loss: 1.9399 - val_accuracy: 0.0267\n",
      "Epoch 2/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.9559 - accuracy: 0.6939 - val_loss: 2.1728 - val_accuracy: 0.0296\n",
      "Epoch 3/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.7035 - accuracy: 0.7784 - val_loss: 2.3953 - val_accuracy: 0.0267\n",
      "Epoch 4/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.5967 - accuracy: 0.8024 - val_loss: 2.6144 - val_accuracy: 0.0267\n",
      "Epoch 5/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.5453 - accuracy: 0.8081 - val_loss: 2.8157 - val_accuracy: 0.0267\n",
      "Epoch 6/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.5012 - accuracy: 0.8279 - val_loss: 2.8097 - val_accuracy: 0.0267\n",
      "Epoch 7/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.4644 - accuracy: 0.8395 - val_loss: 2.7832 - val_accuracy: 0.0267\n",
      "Epoch 8/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.4361 - accuracy: 0.8581 - val_loss: 2.6963 - val_accuracy: 0.0326\n",
      "Epoch 9/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.4141 - accuracy: 0.8576 - val_loss: 2.7148 - val_accuracy: 0.0296\n",
      "Epoch 10/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.3923 - accuracy: 0.8615 - val_loss: 2.5340 - val_accuracy: 0.0415\n",
      "Epoch 11/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.3905 - accuracy: 0.8603 - val_loss: 2.3429 - val_accuracy: 0.0652\n",
      "Epoch 12/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.3745 - accuracy: 0.8635 - val_loss: 2.2705 - val_accuracy: 0.0860\n",
      "Epoch 13/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.3556 - accuracy: 0.8722 - val_loss: 1.9583 - val_accuracy: 0.1502\n",
      "Epoch 14/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.3482 - accuracy: 0.8751 - val_loss: 1.8113 - val_accuracy: 0.2322\n",
      "Epoch 15/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.3212 - accuracy: 0.8825 - val_loss: 1.5776 - val_accuracy: 0.3281\n",
      "Epoch 16/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.3183 - accuracy: 0.8855 - val_loss: 1.3357 - val_accuracy: 0.4130\n",
      "Epoch 17/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.3264 - accuracy: 0.8820 - val_loss: 1.1727 - val_accuracy: 0.5148\n",
      "Epoch 18/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.3311 - accuracy: 0.8838 - val_loss: 1.0802 - val_accuracy: 0.5455\n",
      "Epoch 19/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.2954 - accuracy: 0.8949 - val_loss: 0.8058 - val_accuracy: 0.7085\n",
      "Epoch 20/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.3005 - accuracy: 0.8887 - val_loss: 0.7427 - val_accuracy: 0.7322\n",
      "Epoch 21/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2969 - accuracy: 0.8932 - val_loss: 0.7211 - val_accuracy: 0.7549\n",
      "Epoch 22/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.2901 - accuracy: 0.8949 - val_loss: 0.6379 - val_accuracy: 0.7905\n",
      "Epoch 23/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.2853 - accuracy: 0.9013 - val_loss: 0.6174 - val_accuracy: 0.8004\n",
      "Epoch 24/100\n",
      "4044/4044 [==============================] - 17s 4ms/sample - loss: 0.2833 - accuracy: 0.8976 - val_loss: 0.5476 - val_accuracy: 0.8379\n",
      "Epoch 25/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.2761 - accuracy: 0.8984 - val_loss: 0.6116 - val_accuracy: 0.8182\n",
      "Epoch 26/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.2793 - accuracy: 0.8976 - val_loss: 0.5539 - val_accuracy: 0.8389\n",
      "Epoch 27/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.2574 - accuracy: 0.9060 - val_loss: 0.5867 - val_accuracy: 0.8360\n",
      "Epoch 28/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.2586 - accuracy: 0.9092 - val_loss: 0.5937 - val_accuracy: 0.8449\n",
      "Epoch 29/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2625 - accuracy: 0.9033 - val_loss: 0.5619 - val_accuracy: 0.8508\n",
      "Epoch 30/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2727 - accuracy: 0.9031 - val_loss: 0.6330 - val_accuracy: 0.8152\n",
      "Epoch 31/100\n",
      "4044/4044 [==============================] - 11s 3ms/sample - loss: 0.2633 - accuracy: 0.9033 - val_loss: 0.6348 - val_accuracy: 0.8241\n",
      "Epoch 32/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2618 - accuracy: 0.9060 - val_loss: 0.6346 - val_accuracy: 0.8340\n",
      "Epoch 33/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.2475 - accuracy: 0.9058 - val_loss: 0.6031 - val_accuracy: 0.8498\n",
      "Epoch 34/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.2457 - accuracy: 0.9127 - val_loss: 0.6730 - val_accuracy: 0.8231\n",
      "Epoch 35/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2536 - accuracy: 0.9085 - val_loss: 0.6152 - val_accuracy: 0.8399\n",
      "Epoch 36/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.2423 - accuracy: 0.9105 - val_loss: 0.6052 - val_accuracy: 0.8528\n",
      "Epoch 37/100\n",
      "4044/4044 [==============================] - 14s 4ms/sample - loss: 0.2356 - accuracy: 0.9149 - val_loss: 0.6239 - val_accuracy: 0.8429\n",
      "Epoch 38/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2345 - accuracy: 0.9137 - val_loss: 0.6377 - val_accuracy: 0.8379\n",
      "Epoch 39/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2234 - accuracy: 0.9196 - val_loss: 0.5751 - val_accuracy: 0.8597\n",
      "Epoch 40/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.2296 - accuracy: 0.9137 - val_loss: 0.6472 - val_accuracy: 0.8429\n",
      "Epoch 41/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2252 - accuracy: 0.9194 - val_loss: 0.6189 - val_accuracy: 0.8538\n",
      "Epoch 42/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.2323 - accuracy: 0.9125 - val_loss: 0.6462 - val_accuracy: 0.8379\n",
      "Epoch 43/100\n",
      "4044/4044 [==============================] - 16s 4ms/sample - loss: 0.2376 - accuracy: 0.9157 - val_loss: 0.7294 - val_accuracy: 0.8172\n",
      "Epoch 44/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.2278 - accuracy: 0.9172 - val_loss: 0.6055 - val_accuracy: 0.8557\n",
      "Epoch 45/100\n",
      "4044/4044 [==============================] - 14s 4ms/sample - loss: 0.2152 - accuracy: 0.9238 - val_loss: 0.7003 - val_accuracy: 0.8271\n",
      "Epoch 46/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.2216 - accuracy: 0.9186 - val_loss: 0.7059 - val_accuracy: 0.8320\n",
      "Epoch 47/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.2313 - accuracy: 0.9149 - val_loss: 0.6420 - val_accuracy: 0.8498\n",
      "Epoch 48/100\n",
      "4044/4044 [==============================] - 12s 3ms/sample - loss: 0.2297 - accuracy: 0.9127 - val_loss: 0.6732 - val_accuracy: 0.8350\n",
      "Epoch 49/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.2135 - accuracy: 0.9209 - val_loss: 0.6331 - val_accuracy: 0.8488\n",
      "Epoch 50/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.2156 - accuracy: 0.9231 - val_loss: 0.6576 - val_accuracy: 0.8429\n",
      "Epoch 51/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.2121 - accuracy: 0.9246 - val_loss: 0.6581 - val_accuracy: 0.8617\n",
      "Epoch 52/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.2069 - accuracy: 0.9231 - val_loss: 0.7701 - val_accuracy: 0.8132\n",
      "Epoch 53/100\n",
      "4044/4044 [==============================] - 10s 3ms/sample - loss: 0.2226 - accuracy: 0.9194 - val_loss: 0.6034 - val_accuracy: 0.8587\n",
      "Epoch 54/100\n",
      "4044/4044 [==============================] - 10s 3ms/sample - loss: 0.2139 - accuracy: 0.9211 - val_loss: 0.6662 - val_accuracy: 0.8538\n",
      "Epoch 55/100\n",
      "4044/4044 [==============================] - 11s 3ms/sample - loss: 0.2040 - accuracy: 0.9211 - val_loss: 0.7467 - val_accuracy: 0.8152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.1862 - accuracy: 0.9318 - val_loss: 0.6504 - val_accuracy: 0.8528\n",
      "Epoch 57/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.1928 - accuracy: 0.9285 - val_loss: 0.6475 - val_accuracy: 0.8518\n",
      "Epoch 58/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.2203 - accuracy: 0.9206 - val_loss: 0.6806 - val_accuracy: 0.8468\n",
      "Epoch 59/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.2073 - accuracy: 0.9214 - val_loss: 0.7010 - val_accuracy: 0.8330\n",
      "Epoch 60/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.1987 - accuracy: 0.9258 - val_loss: 0.6805 - val_accuracy: 0.8538\n",
      "Epoch 61/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.1945 - accuracy: 0.9293 - val_loss: 0.6511 - val_accuracy: 0.8636\n",
      "Epoch 62/100\n",
      "4044/4044 [==============================] - 11s 3ms/sample - loss: 0.1940 - accuracy: 0.9293 - val_loss: 0.7472 - val_accuracy: 0.8241\n",
      "Epoch 63/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.1893 - accuracy: 0.9322 - val_loss: 0.6658 - val_accuracy: 0.8478\n",
      "Epoch 64/100\n",
      "4044/4044 [==============================] - 10s 3ms/sample - loss: 0.1934 - accuracy: 0.9283 - val_loss: 0.7104 - val_accuracy: 0.8370\n",
      "Epoch 65/100\n",
      "4044/4044 [==============================] - 10s 3ms/sample - loss: 0.1805 - accuracy: 0.9350 - val_loss: 0.6788 - val_accuracy: 0.8498\n",
      "Epoch 66/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.1912 - accuracy: 0.9313 - val_loss: 0.6383 - val_accuracy: 0.8577\n",
      "Epoch 67/100\n",
      "4044/4044 [==============================] - 13s 3ms/sample - loss: 0.1746 - accuracy: 0.9372 - val_loss: 0.7398 - val_accuracy: 0.8261\n",
      "Epoch 68/100\n",
      "4044/4044 [==============================] - 14s 4ms/sample - loss: 0.1837 - accuracy: 0.9342 - val_loss: 0.6699 - val_accuracy: 0.8528\n",
      "Epoch 69/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.1887 - accuracy: 0.9290 - val_loss: 0.8258 - val_accuracy: 0.8123\n",
      "Epoch 70/100\n",
      "4044/4044 [==============================] - 15s 4ms/sample - loss: 0.1936 - accuracy: 0.9290 - val_loss: 0.6771 - val_accuracy: 0.8488\n",
      "Epoch 71/100\n",
      "4044/4044 [==============================] - 14s 3ms/sample - loss: 0.1910 - accuracy: 0.9305 - val_loss: 0.7105 - val_accuracy: 0.8409\n",
      "Epoch 72/100\n",
      "4044/4044 [==============================] - 10s 3ms/sample - loss: 0.1743 - accuracy: 0.9387 - val_loss: 0.6920 - val_accuracy: 0.8557\n",
      "Epoch 73/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.1739 - accuracy: 0.9364 - val_loss: 0.6732 - val_accuracy: 0.8538\n",
      "Epoch 74/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1716 - accuracy: 0.9364 - val_loss: 0.7182 - val_accuracy: 0.8439\n",
      "Epoch 75/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1800 - accuracy: 0.9362 - val_loss: 0.7203 - val_accuracy: 0.8547\n",
      "Epoch 76/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1829 - accuracy: 0.9325 - val_loss: 0.7122 - val_accuracy: 0.8419\n",
      "Epoch 77/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1648 - accuracy: 0.9431 - val_loss: 0.7549 - val_accuracy: 0.8399\n",
      "Epoch 78/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1715 - accuracy: 0.9379 - val_loss: 0.7449 - val_accuracy: 0.8449\n",
      "Epoch 79/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.1625 - accuracy: 0.9394 - val_loss: 0.7141 - val_accuracy: 0.8488\n",
      "Epoch 80/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1603 - accuracy: 0.9441 - val_loss: 0.7347 - val_accuracy: 0.8399\n",
      "Epoch 81/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1608 - accuracy: 0.9397 - val_loss: 0.7648 - val_accuracy: 0.8340\n",
      "Epoch 82/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1629 - accuracy: 0.9384 - val_loss: 0.7241 - val_accuracy: 0.8419\n",
      "Epoch 83/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1627 - accuracy: 0.9414 - val_loss: 0.7295 - val_accuracy: 0.8409\n",
      "Epoch 84/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1486 - accuracy: 0.9473 - val_loss: 0.6920 - val_accuracy: 0.8518\n",
      "Epoch 85/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1405 - accuracy: 0.9488 - val_loss: 0.7330 - val_accuracy: 0.8370\n",
      "Epoch 86/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1597 - accuracy: 0.9431 - val_loss: 0.6844 - val_accuracy: 0.8478\n",
      "Epoch 87/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1453 - accuracy: 0.9446 - val_loss: 0.6885 - val_accuracy: 0.8538\n",
      "Epoch 88/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1508 - accuracy: 0.9458 - val_loss: 0.7189 - val_accuracy: 0.8557\n",
      "Epoch 89/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1384 - accuracy: 0.9503 - val_loss: 0.7603 - val_accuracy: 0.8449\n",
      "Epoch 90/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1525 - accuracy: 0.9458 - val_loss: 0.8178 - val_accuracy: 0.8330\n",
      "Epoch 91/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1498 - accuracy: 0.9456 - val_loss: 0.7881 - val_accuracy: 0.8449\n",
      "Epoch 92/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1566 - accuracy: 0.9431 - val_loss: 0.7491 - val_accuracy: 0.8508\n",
      "Epoch 93/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1475 - accuracy: 0.9424 - val_loss: 0.7736 - val_accuracy: 0.8458\n",
      "Epoch 94/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1475 - accuracy: 0.9446 - val_loss: 0.7748 - val_accuracy: 0.8478\n",
      "Epoch 95/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1483 - accuracy: 0.9473 - val_loss: 0.8172 - val_accuracy: 0.8291\n",
      "Epoch 96/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1426 - accuracy: 0.9476 - val_loss: 0.7631 - val_accuracy: 0.8528\n",
      "Epoch 97/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1421 - accuracy: 0.9456 - val_loss: 0.7773 - val_accuracy: 0.8399\n",
      "Epoch 98/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1343 - accuracy: 0.9540 - val_loss: 0.7898 - val_accuracy: 0.8409\n",
      "Epoch 99/100\n",
      "4044/4044 [==============================] - 10s 2ms/sample - loss: 0.1439 - accuracy: 0.9471 - val_loss: 0.7197 - val_accuracy: 0.8557\n",
      "Epoch 100/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 0.1515 - accuracy: 0.9451 - val_loss: 0.7711 - val_accuracy: 0.8468\n",
      "1265/1265 [==============================] - 1s 640us/sample - loss: 0.9807 - accuracy: 0.7747\n",
      "test_loss: 0.980686, accuracy: 0.774704\n"
     ]
    }
   ],
   "source": [
    "x_train = data.trainReviews\n",
    "y_train = data.trainLabels\n",
    "x_eval = data.evalReviews\n",
    "y_eval = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('./bilstm_attention/best_model/model_{epoch:02d}-{val_accuracy:.2f}.hdf5', save_best_only=True, save_weights_only=True)\n",
    "# history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.3,shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True)\n",
    "\n",
    "#验证\n",
    "\n",
    "scores = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "#保存模型\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./bilstm_attention/bilstm_attention.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "model.save_weights('./bilstm_attention/bilstm_attention.h5')\n",
    "\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
