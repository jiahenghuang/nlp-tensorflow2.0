{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Python/3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import BatchNormalization,Layer,Input,Conv2D,MaxPool2D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape,LSTM\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import multiprocessing\n",
    "import yaml\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    #数据集路径\n",
    "    dataSource = \"../marriage.txt\"\n",
    "    stopWordSource = \"../stopword.txt\"\n",
    "    #分词后保留大于等于最低词频的词\n",
    "    miniFreq=1\n",
    "    #统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n",
    "    sequenceLength = 30 \n",
    "    batchSize=128\n",
    "    epochs=100\n",
    "    numClasses = 5\n",
    "    #训练集的比例\n",
    "    rate = 0.8  \n",
    "    #生成嵌入词向量的维度\n",
    "    embeddingSize = 200\n",
    "    #卷积核数\n",
    "    numFilters = 128\n",
    "    #卷积核大小\n",
    "    filterSizes = [1,2,3,4,5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    #L2正则系数\n",
    "    l2RegLambda = 0.01\n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文语料\n",
    "#设置输出日志\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "file = open(\"../marriage.txt\") \n",
    "sentences=[]\n",
    "with open('../marriage.txt') as fr:\n",
    "    for line in fr.readlines():\n",
    "        temp=line.strip().split('\\t')\n",
    "#         sentences.append(jieba.lcut(temp[0]))\n",
    "        sentences.append(list(temp[0]))\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,size=config.embeddingSize,\n",
    "                     min_count=config.miniFreq,\n",
    "                     window=10,\n",
    "                     workers=multiprocessing.cpu_count(),sg=1,\n",
    "                     iter=20)\n",
    "model.save('../word2VecModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['嗯', '有', '啊', '反', '了', '看', '没', '那', '太', '好', '几', '年', '零', '四', '是', '的', '我', '现', '在', '边', '上', '客', '户', '哎', '你', '稍', '等', '一', '下', '先', '过', '挂', '掉', '这', '还', '办', '最', '久', '都', '两', '班', '可', '能', '次', '再', '完', '跟', '他', '见', '多', '认', '证', '就', '问', '题', '但', '三', '报', '考', '交', '离', '婚', '拿', '事', '时', '候', '直', '接', '今', '天', '晚', '早', '走', '对', '吧', '大', '米', '然', '后', '弄', '回', '来', '说', '让', '钱', '吗', '号', '厅', '七', '呀', '听', '很', '吵', '份', '整', '个', '拜', '哇', '倒', '投', '诉', '不', '单', '身', '陈', '倩', '豆', '瓣', '带', '放', '啦', '给', '留', '线', '人', '方', '便', '答', '哈', '丢', '八', '前', '哦', '换', '正', '版', '呢', '半', '怪', '家', '里', '面', '出', '全', '呃', '妹', '找', '司', '法', '着', '急', '电', '话', '行', '选', '咋', '肯', '定', '改', '搬', '总', '共', '念', '奇', '意', '思', '已', '经', '退', '进', '确', '到', '们', '协', '议', '书', '章', '明', '点', '每', '坐', '车', '把', '错', '包', '括', '强', '调', '喂', '去', '也', '干', '啥', '帮', '吓', '跳', '常', '声', '开', '相', '爱', '午', '打', '加', '油', '会', '玩', '咱', '聊', '二', '十', '目', '些', '讲', '想', '旅', '游', '九', '猫', '跑', '姐', '成', '满', '网', '怕', '种', '工', '作', '卡', '坏', '老', '外', '嘛', '官', '什', '么', '院', '解', '只', '裁', '决', '诶', '哟', '谢', '您', '记', '要', '提', '当', '关', '站', '结', '束', '以', '难', '铺', '五', '农', '水', '平', '礼', '因', '为', '间', '志', '友', '别', '北', '京', '搞', '李', '峰', '镇', '唉', '牌', '道', '知', '品', '真', '呗', '样', '哪', '珍', '姥', '手', '月', '卫', '辉', '之', '发', '谁', '派', '暂', '用', '长', '案', '照', '玛', '像', '注', '册', '小', '阵', '算', '所', '示', '检', '体', '谅', '饿', '抢', '怎', '刚', '刘', '文', '必', '春', '节', '六', '判', '摆', '骗', '室', '件', '妈', '铁', '岭', '沈', '阳', '东', '西', '邮', '箱', '处', '吃', '饭', '得', '婆', '本', '登', '蚊', '子', '部', '分', '赵', '导', '爸', '居', '幺', '噢', '优', '雅', '应', '蚂', '蚁', '嫂', '钟', '嘞', '淘', '宝', '薄', '情', '传', '票', '空', '供', '签', '才', '卸', '载', '力', '量', '起', '讼', '仔', '细', '更', '深', '录', '懂', '娃', '服', '务', '果', '浩', '写', '概', '临', '汾', '触', '少', '该', '底', '随', '往', '员', '麻', '烦', '资', '料', '销', '快', '递', '同', '凌', '晨', '地', '朋', '场', '售', '张', '白', '纸', '红', '清', '楚', '枣', '待', '拍', '鸡', '呵', '期', '哼', '安', '排', '亲', '觉', '需', '语', 'w', 'o', 'r', 'd', '根', '邻', '向', '信', '新', '娘', '差', '公', '儿', '伴', '战', '播', '星', '万', '另', '支', '持', '于', '孩', '理', '杜', '位', '律', '效', '核', '实', '昨', '中', '名', '申', '虑', '自', '己', '区', '县', '联', '系', '例', '迁', '移', '转', '舞', '蹈', '马', '除', 's', '企', '业', '账', '连', '革', '翻', '近', '男', '挺', '轻', '如', '补', '字', '贵', '州', '请', '害', '冯', '燕', '花', '朵', '填', '鞋', '告', '烊', '比', '惠', '香', '港', '寄', '具', '益', '鸿', '乐', '抱', '它', '叫', '影', '响', '初', '江', '苏', '域', '英', '绑', '瞬', '洪', '城', '并', '姻', '皆', '欢', '喜', '南', '台', '湾', '段', '疯', '狂', '码', '通', 'v', 'i', 'p', '牙', '医', '周', '或', '者', '又', '女', '生', '撤', '头', '涨', '牛', '舒', '辛', '苦', '扰', '准', '备', '银', '机', '主', '动', '活', '兜', '国', '晓', '使', '斗', '罗', '陆', '楼', '验', '草', '阶', '杨', '凉', '希', '望', '广', '穿', '从', '压', '观', '众', '政', '升', '级', '摄', '合', '代', '阿', '装', '睡', '梅', '师', '查', '而', '视', '频', '遭', '遇', '淡', '顺', '骏', '纯', '盘', '岁', '基', '建', '忙', '试', '患', '聪', '做', '魔', '高', '担', '心', '呐', '棒', '千', '重', '妞', '灵', '魂', '晕', '般', '迷', '汕', '感', '阴', '风', '无', '锋', 'k', '操', '领', '翰', '未', '菜', '石', '庄', '嗳', '困', '碰', '壮', '局', '逃', '避', '路', '指', '变', '日', '卖', '复', '杂', '紧', '取', '消', '废', '征', '含', '蛋', '受', '限', '毛', '唱', '学', '历', '遗', '忘', '慢', '咯', '材', '值', '据', '神', '病', '策', '丧', '偶', '续', '缘', '尝', '增', '弹', '撑', '订', '百', '元', '贷', '士', '斥', '论', '骂', '闷', '火', '庭', '首', '纠', '纷', '黄', '混', '尺', '寸', '洲', '美', '和', '芦', '荟', '胶', '霉', '货', '按', '层', '简', '靠', '板', '伤', '庆', '片', '曾', '死', '估', '计', '滚', '送', '罢', '笑', '洗', '澡', '其', '蛮', '歉', '击', '介', '绍', '碌', '民', '丰', '利', '既', '口', '鸭', '势', '股', '买', '逗', '傍', '她', '善', '表', '歌', '修', '房', '盐', '炒', '顾', '喽', '抓', '爆', '态', '度', '适', '且', '内', '杭', '梨', '青', '山', '4', '0', '丽', '左', '右', '印', '设', '置', '兆', '类', '拨', '款', '金', '额', '借', '扣', '任', '娜', '扔', '须', '吕', '旁', '赔', '流', '犯', '愤', '范', '被', '存', '乱', '呦', '异', '球', '丶', '求', '赶', '键', '断', '冻', '骑', '鬼', '秒', '研', '究', '纪', '显', '润', '貌', '似', '宁', '妥', '微', '特', '门', '铃', '收', '令', '圈', '教', '推', '读', '脚', '鸣', '智', '桂', '梦', '商', '亮', '付', '详', '攻', '闭', '失', '占', '保', '佑', '拖', '况', '横', '斩', '队', '耶', '馈', '费', '噻', '术', '扮', '姚', '甲', '逼', '汉', '易', '败', '套', '举', '档', '拌', '溜', '冰', '贴', '膜', '王', '拥', '泵', '厂', '象', '坤', '伯', '杰', '俊', '际', '气', '挣', '参', '假', '删', '醒', '雁', '音', '状', '毁', '软', '赚', '器', '浪', '招', '圳', '住', '挖', '掘', '称', '惨', '邓', '块', '毕', '竟', '咽', '唤', '欠', '戏', '呼', '华', '泰', '遍', '闻', '控', '积', '散', '均', '替', '贾', '玲', '笔', '贩', '欧', '荐', '权', '课', '番', '偷', '购', '程', '卦', 'a', ' ', '巴', '徐', '尽', '越', '够', '浓', '黎', '宏', '振', '怀', '疑', '科', '评', '哥', '维', '羞', '批', '戚', '双', '秋', '柿', '汤', '狗', '胖', '输', '化', '绩', '端', '入', '丁', '伟', '灾', '沙', '烧', '拔', '谓', '绿', '静', '盖', '袋', '府', '炮', '嫁', '嚎', '咨', '询', '团', '密', '栋', '钥', '匙', '嘟', '努', '原', '质', '启', '吞', '噬', '云', '焊', '访', '店', '康', '海', '扫', '释', '审', '价', '粘', '谈', '致', '较', '糖', '校', '宽', '卷', '列', '嗨', '艺', '耽', '搁', '第', '标', '林', '朝', '市', 'q', '独', '立', '由', '樱', '童', '杠', '捎', '谎', '言', '旧', '达', '湿', '探', '讨', '震', '迟', '速', '沟', '性', '虽', '闺', '夏', '远', '与', '布', '捺', '始', '夜', '兴', '索', '脸', '嘲', '赖', '及', '匹', '配', '雯', '座', '灯', '属', '监', '猜', '统', '厉', '锁', '型', '舔', '尾', '隐', '私', '误', '荤', '巾', '何', '浏', '览', '颜', '药', '映', '约', '骚', '渠', '拾', '践', '奢', '侈', '良', '绝', '各', '稿', '架', '圣', '剑', '妆', '划', '雨', '奖', '励', '险', '式', '透', '露', '姓', '析', '愿', '孙', '刷', '涂', '黑', '休', '息', '创', '搜', '咸', '呆', '婉', '管', '条', '闲', '添', '厕', '钻', '促', '光', '纤', 'm', '追', '余', '郁', '伙', '链', '助', '预', '父', '尔', '运', '刻', '侍', '形', '乳', '眼', '功', '累', '凭', '测', '傻', '脑', '扯', '糟', '充', '昌', '址', '诞', '烟', '物', '帐', '省', '豪', '颁', '诈', '专', '框', '昏', '僭', 'y', '齐', '凤', '凰', '瓦', '温', '瑞', '昆', '图', '甘', '肃', '劝', '肛', '扎', '拉', '拼', '世', '湖', '故'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 6321 6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.dataSource = config.dataSource\n",
    "        self.stopWordSource = config.stopWordSource  \n",
    "        # 每条输入的序列处理为定长\n",
    "        self.sequenceLength = config.sequenceLength  \n",
    "        self.embeddingSize = config.embeddingSize\n",
    "        self.rate = config.rate\n",
    "        self.miniFreq=config.miniFreq\n",
    "        self.stopWordDict = {}\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding =None\n",
    "        self.n_symbols=0\n",
    "        self.wordToIndex = {}\n",
    "        self.indexToWord = {}\n",
    "        \n",
    "    def readData(self, filePath):\n",
    "        text=[]\n",
    "        label=[]\n",
    "        with open(filePath) as fr:\n",
    "            for line in file:\n",
    "                temp=line.strip().split('\\t')\n",
    "                text.append(temp[0])\n",
    "                label.append(temp[1])\n",
    "        print('data:',len(text),len(label))\n",
    "#         texts = [jieba.cut(document) for document in text]\n",
    "        texts = [list(document) for document in text]\n",
    "        return texts, label\n",
    "    \n",
    "    def readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    def getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        #中文\n",
    "        model = gensim.models.Word2Vec.load('../word2VecModel')\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        wordEmbedding.append(np.zeros(self.embeddingSize))\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self.embeddingSize))\n",
    "        for word in words:\n",
    "            vector =model[word]\n",
    "            vocab.append(word)\n",
    "            wordEmbedding.append(vector)           \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        #去掉停用词\n",
    "#         subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        #统计词频，排序\n",
    "#         wordCount = Counter(subWords)\n",
    "        wordCount = Counter(allWords)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        #去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= self.miniFreq ]\n",
    "        #获取词列表和顺序对应的预训练权重矩阵\n",
    "        vocab, wordEmbedding = self.getWordEmbedding(words)\n",
    "        \n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self.wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self.indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        self.n_symbols = len(self.wordToIndex) + 1\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.wordToIndex, f)\n",
    "        with open(\"../wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.indexToWord, f)\n",
    "            \n",
    "    def reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论里面的词，根据词表，映射为index表示\n",
    "        每条评论 用index组成的定长数组来表示\n",
    "        \"\"\"\n",
    "        review=list(review)\n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "        return reviewVec\n",
    "\n",
    "    def genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self.reviewProcess(x[i], self.sequenceLength, self.wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            labels.append([y[i]])    \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        #trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        #evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "         \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        #读取停用词\n",
    "        self.readStopWord(self.stopWordSource)\n",
    "        #读取数据集\n",
    "        reviews, labels = self.readData(self.dataSource)\n",
    "        #分词、去停用词\n",
    "        #生成 词汇-索引 映射表和预训练权重矩阵，并保存\n",
    "        self.genVocabulary(reviews)\n",
    "        #初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self.genTrainEvalData(reviews, labels, self.rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Python/3.7/site-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 200)           236200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 30, 100)           100400    \n",
      "_________________________________________________________________\n",
      "attention_layer (AttentionLa (None, 100)               930       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 338,536\n",
      "Trainable params: 338,336\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = backend.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = backend.softmax(backend.tanh(backend.dot(x, self.W) + self.b))\n",
    "        outputs = backend.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = backend.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "\n",
    "\n",
    "def train_lstm(n_symbols,embedding_weights,config):\n",
    "    \n",
    "    model =Sequential([\n",
    "        Embedding(input_dim=n_symbols, output_dim=config.embeddingSize,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=config.sequenceLength),\n",
    "        \n",
    "    #LSTM层\n",
    "    Dropout(config.dropoutKeepProb),\n",
    "    Bidirectional(LSTM(200,activation='tanh', dropout=0.5, recurrent_dropout=0.5,return_sequences=True), merge_mode='concat'),\n",
    "    AttentionLayer(),\n",
    "    Dropout(config.dropoutKeepProb),\n",
    "    BatchNormalization(),\n",
    "    Dense(6, activation='softmax')])  \n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                 loss=losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "model = train_lstm(n_symbols,wordEmbedding,config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/100\n",
      "4044/4044 [==============================] - 9s 2ms/sample - loss: 1.8755 - accuracy: 0.2814 - val_loss: 1.8720 - val_accuracy: 0.0267\n",
      "Epoch 2/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 1.2566 - accuracy: 0.5685 - val_loss: 1.9489 - val_accuracy: 0.0267\n",
      "Epoch 3/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.9053 - accuracy: 0.7238 - val_loss: 2.0614 - val_accuracy: 0.0267\n",
      "Epoch 4/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.7028 - accuracy: 0.7839 - val_loss: 2.1771 - val_accuracy: 0.0287\n",
      "Epoch 5/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.6266 - accuracy: 0.8027 - val_loss: 2.3142 - val_accuracy: 0.0296\n",
      "Epoch 6/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.5623 - accuracy: 0.8200 - val_loss: 2.3383 - val_accuracy: 0.0306\n",
      "Epoch 7/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.5214 - accuracy: 0.8304 - val_loss: 2.4065 - val_accuracy: 0.0336\n",
      "Epoch 8/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.4843 - accuracy: 0.8422 - val_loss: 2.3068 - val_accuracy: 0.0366\n",
      "Epoch 9/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.4452 - accuracy: 0.8519 - val_loss: 2.2120 - val_accuracy: 0.0543\n",
      "Epoch 10/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.4480 - accuracy: 0.8504 - val_loss: 2.1635 - val_accuracy: 0.0702\n",
      "Epoch 11/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.4095 - accuracy: 0.8586 - val_loss: 1.9521 - val_accuracy: 0.1186\n",
      "Epoch 12/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.3929 - accuracy: 0.8672 - val_loss: 1.8492 - val_accuracy: 0.1739\n",
      "Epoch 13/100\n",
      "4044/4044 [==============================] - 5s 1ms/sample - loss: 0.3974 - accuracy: 0.8618 - val_loss: 1.7048 - val_accuracy: 0.2381\n",
      "Epoch 14/100\n",
      "4044/4044 [==============================] - 6s 1ms/sample - loss: 0.3641 - accuracy: 0.8771 - val_loss: 1.2868 - val_accuracy: 0.4101\n",
      "Epoch 15/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3614 - accuracy: 0.8769 - val_loss: 1.1852 - val_accuracy: 0.4694\n",
      "Epoch 16/100\n",
      "4044/4044 [==============================] - 4s 988us/sample - loss: 0.3493 - accuracy: 0.8778 - val_loss: 0.9780 - val_accuracy: 0.5840\n",
      "Epoch 17/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3505 - accuracy: 0.8754 - val_loss: 0.9154 - val_accuracy: 0.6196\n",
      "Epoch 18/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3325 - accuracy: 0.8853 - val_loss: 0.7559 - val_accuracy: 0.7184\n",
      "Epoch 19/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3170 - accuracy: 0.8919 - val_loss: 0.6706 - val_accuracy: 0.7540\n",
      "Epoch 20/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3281 - accuracy: 0.8843 - val_loss: 0.6127 - val_accuracy: 0.7875\n",
      "Epoch 21/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.3110 - accuracy: 0.8875 - val_loss: 0.5960 - val_accuracy: 0.7964\n",
      "Epoch 22/100\n",
      "4044/4044 [==============================] - 4s 966us/sample - loss: 0.3007 - accuracy: 0.8917 - val_loss: 0.5531 - val_accuracy: 0.8192\n",
      "Epoch 23/100\n",
      "4044/4044 [==============================] - 4s 999us/sample - loss: 0.3030 - accuracy: 0.8882 - val_loss: 0.6256 - val_accuracy: 0.7994\n",
      "Epoch 24/100\n",
      "4044/4044 [==============================] - 4s 964us/sample - loss: 0.2905 - accuracy: 0.8984 - val_loss: 0.5860 - val_accuracy: 0.8182\n",
      "Epoch 25/100\n",
      "4044/4044 [==============================] - 4s 934us/sample - loss: 0.2954 - accuracy: 0.8919 - val_loss: 0.5483 - val_accuracy: 0.8340\n",
      "Epoch 26/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.2891 - accuracy: 0.8956 - val_loss: 0.4996 - val_accuracy: 0.8478\n",
      "Epoch 27/100\n",
      "4044/4044 [==============================] - 4s 944us/sample - loss: 0.2801 - accuracy: 0.9011 - val_loss: 0.5567 - val_accuracy: 0.8370\n",
      "Epoch 28/100\n",
      "4044/4044 [==============================] - 4s 969us/sample - loss: 0.2893 - accuracy: 0.8927 - val_loss: 0.6093 - val_accuracy: 0.8152\n",
      "Epoch 29/100\n",
      "4044/4044 [==============================] - 4s 986us/sample - loss: 0.2823 - accuracy: 0.8979 - val_loss: 0.6225 - val_accuracy: 0.8241\n",
      "Epoch 30/100\n",
      "4044/4044 [==============================] - 4s 976us/sample - loss: 0.2769 - accuracy: 0.9031 - val_loss: 0.5662 - val_accuracy: 0.8379\n",
      "Epoch 31/100\n",
      "4044/4044 [==============================] - 4s 968us/sample - loss: 0.2648 - accuracy: 0.9058 - val_loss: 0.5535 - val_accuracy: 0.8409\n",
      "Epoch 32/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.2646 - accuracy: 0.9031 - val_loss: 0.5116 - val_accuracy: 0.8597\n",
      "Epoch 33/100\n",
      "4044/4044 [==============================] - 4s 961us/sample - loss: 0.2582 - accuracy: 0.9063 - val_loss: 0.5472 - val_accuracy: 0.8508\n",
      "Epoch 34/100\n",
      "4044/4044 [==============================] - 4s 954us/sample - loss: 0.2688 - accuracy: 0.9043 - val_loss: 0.5458 - val_accuracy: 0.8587\n",
      "Epoch 35/100\n",
      "4044/4044 [==============================] - 4s 972us/sample - loss: 0.2577 - accuracy: 0.9073 - val_loss: 0.5624 - val_accuracy: 0.8538\n",
      "Epoch 36/100\n",
      "4044/4044 [==============================] - 4s 971us/sample - loss: 0.2655 - accuracy: 0.9033 - val_loss: 0.6257 - val_accuracy: 0.8310\n",
      "Epoch 37/100\n",
      "4044/4044 [==============================] - 4s 961us/sample - loss: 0.2523 - accuracy: 0.9068 - val_loss: 0.6189 - val_accuracy: 0.8419\n",
      "Epoch 38/100\n",
      "4044/4044 [==============================] - 4s 974us/sample - loss: 0.2596 - accuracy: 0.9110 - val_loss: 0.6229 - val_accuracy: 0.8300\n",
      "Epoch 39/100\n",
      "4044/4044 [==============================] - 4s 957us/sample - loss: 0.2590 - accuracy: 0.9090 - val_loss: 0.5799 - val_accuracy: 0.8478\n",
      "Epoch 40/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.2524 - accuracy: 0.9073 - val_loss: 0.5815 - val_accuracy: 0.8409\n",
      "Epoch 41/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.2513 - accuracy: 0.9063 - val_loss: 0.5992 - val_accuracy: 0.8370\n",
      "Epoch 42/100\n",
      "4044/4044 [==============================] - 4s 960us/sample - loss: 0.2530 - accuracy: 0.9050 - val_loss: 0.6414 - val_accuracy: 0.8291\n",
      "Epoch 43/100\n",
      "4044/4044 [==============================] - 4s 986us/sample - loss: 0.2410 - accuracy: 0.9152 - val_loss: 0.6038 - val_accuracy: 0.8419\n",
      "Epoch 44/100\n",
      "4044/4044 [==============================] - 4s 943us/sample - loss: 0.2399 - accuracy: 0.9174 - val_loss: 0.5743 - val_accuracy: 0.8498\n",
      "Epoch 45/100\n",
      "4044/4044 [==============================] - 4s 960us/sample - loss: 0.2471 - accuracy: 0.9139 - val_loss: 0.5790 - val_accuracy: 0.8488\n",
      "Epoch 46/100\n",
      "4044/4044 [==============================] - 4s 928us/sample - loss: 0.2419 - accuracy: 0.9139 - val_loss: 0.6111 - val_accuracy: 0.8409\n",
      "Epoch 47/100\n",
      "4044/4044 [==============================] - 4s 973us/sample - loss: 0.2360 - accuracy: 0.9159 - val_loss: 0.5946 - val_accuracy: 0.8528\n",
      "Epoch 48/100\n",
      "4044/4044 [==============================] - 4s 924us/sample - loss: 0.2450 - accuracy: 0.9102 - val_loss: 0.6678 - val_accuracy: 0.8291\n",
      "Epoch 49/100\n",
      "4044/4044 [==============================] - 4s 934us/sample - loss: 0.2325 - accuracy: 0.9206 - val_loss: 0.6198 - val_accuracy: 0.8409\n",
      "Epoch 50/100\n",
      "4044/4044 [==============================] - 4s 892us/sample - loss: 0.2298 - accuracy: 0.9167 - val_loss: 0.6410 - val_accuracy: 0.8340\n",
      "Epoch 51/100\n",
      "4044/4044 [==============================] - 4s 934us/sample - loss: 0.2258 - accuracy: 0.9186 - val_loss: 0.6469 - val_accuracy: 0.8330\n",
      "Epoch 52/100\n",
      "4044/4044 [==============================] - 4s 939us/sample - loss: 0.2241 - accuracy: 0.9157 - val_loss: 0.6054 - val_accuracy: 0.8538\n",
      "Epoch 53/100\n",
      "4044/4044 [==============================] - 4s 978us/sample - loss: 0.2274 - accuracy: 0.9191 - val_loss: 0.6261 - val_accuracy: 0.8389\n",
      "Epoch 54/100\n",
      "4044/4044 [==============================] - 4s 936us/sample - loss: 0.2293 - accuracy: 0.9125 - val_loss: 0.6007 - val_accuracy: 0.8547\n",
      "Epoch 55/100\n",
      "4044/4044 [==============================] - 4s 949us/sample - loss: 0.2246 - accuracy: 0.9206 - val_loss: 0.6031 - val_accuracy: 0.8449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "4044/4044 [==============================] - 4s 976us/sample - loss: 0.2273 - accuracy: 0.9204 - val_loss: 0.6039 - val_accuracy: 0.8478\n",
      "Epoch 57/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.2248 - accuracy: 0.9174 - val_loss: 0.6433 - val_accuracy: 0.8340\n",
      "Epoch 58/100\n",
      "4044/4044 [==============================] - 4s 965us/sample - loss: 0.2242 - accuracy: 0.9211 - val_loss: 0.5729 - val_accuracy: 0.8597\n",
      "Epoch 59/100\n",
      "4044/4044 [==============================] - 4s 982us/sample - loss: 0.2164 - accuracy: 0.9189 - val_loss: 0.6012 - val_accuracy: 0.8488\n",
      "Epoch 60/100\n",
      "4044/4044 [==============================] - 4s 955us/sample - loss: 0.2105 - accuracy: 0.9196 - val_loss: 0.5996 - val_accuracy: 0.8528\n",
      "Epoch 61/100\n",
      "4044/4044 [==============================] - 4s 932us/sample - loss: 0.2100 - accuracy: 0.9238 - val_loss: 0.6198 - val_accuracy: 0.8468\n",
      "Epoch 62/100\n",
      "4044/4044 [==============================] - 4s 945us/sample - loss: 0.2185 - accuracy: 0.9241 - val_loss: 0.6662 - val_accuracy: 0.8409\n",
      "Epoch 63/100\n",
      "4044/4044 [==============================] - 4s 952us/sample - loss: 0.2227 - accuracy: 0.9204 - val_loss: 0.6322 - val_accuracy: 0.8439\n",
      "Epoch 64/100\n",
      "4044/4044 [==============================] - 4s 931us/sample - loss: 0.2165 - accuracy: 0.9182 - val_loss: 0.6757 - val_accuracy: 0.8271\n",
      "Epoch 65/100\n",
      "4044/4044 [==============================] - 4s 918us/sample - loss: 0.2087 - accuracy: 0.9228 - val_loss: 0.6306 - val_accuracy: 0.8399\n",
      "Epoch 66/100\n",
      "4044/4044 [==============================] - 4s 974us/sample - loss: 0.2027 - accuracy: 0.9310 - val_loss: 0.6109 - val_accuracy: 0.8607\n",
      "Epoch 67/100\n",
      "4044/4044 [==============================] - 4s 930us/sample - loss: 0.2114 - accuracy: 0.9251 - val_loss: 0.6425 - val_accuracy: 0.8478\n",
      "Epoch 68/100\n",
      "4044/4044 [==============================] - 4s 922us/sample - loss: 0.2088 - accuracy: 0.9263 - val_loss: 0.6900 - val_accuracy: 0.8360\n",
      "Epoch 69/100\n",
      "4044/4044 [==============================] - 4s 926us/sample - loss: 0.2130 - accuracy: 0.9243 - val_loss: 0.6398 - val_accuracy: 0.8439\n",
      "Epoch 70/100\n",
      "4044/4044 [==============================] - 4s 904us/sample - loss: 0.2022 - accuracy: 0.9266 - val_loss: 0.6366 - val_accuracy: 0.8389\n",
      "Epoch 71/100\n",
      "4044/4044 [==============================] - 4s 991us/sample - loss: 0.2035 - accuracy: 0.9258 - val_loss: 0.6393 - val_accuracy: 0.8439\n",
      "Epoch 72/100\n",
      "4044/4044 [==============================] - 4s 987us/sample - loss: 0.2071 - accuracy: 0.9268 - val_loss: 0.6262 - val_accuracy: 0.8468\n",
      "Epoch 73/100\n",
      "4044/4044 [==============================] - 4s 995us/sample - loss: 0.1961 - accuracy: 0.9285 - val_loss: 0.6274 - val_accuracy: 0.8449\n",
      "Epoch 74/100\n",
      "4044/4044 [==============================] - 4s 976us/sample - loss: 0.2007 - accuracy: 0.9278 - val_loss: 0.6090 - val_accuracy: 0.8547\n",
      "Epoch 75/100\n",
      "4044/4044 [==============================] - 4s 973us/sample - loss: 0.1870 - accuracy: 0.9305 - val_loss: 0.6310 - val_accuracy: 0.8468\n",
      "Epoch 76/100\n",
      "4044/4044 [==============================] - 4s 966us/sample - loss: 0.2088 - accuracy: 0.9206 - val_loss: 0.6205 - val_accuracy: 0.8547\n",
      "Epoch 77/100\n",
      "4044/4044 [==============================] - 4s 970us/sample - loss: 0.2127 - accuracy: 0.9191 - val_loss: 0.6482 - val_accuracy: 0.8468\n",
      "Epoch 78/100\n",
      "4044/4044 [==============================] - 4s 964us/sample - loss: 0.1936 - accuracy: 0.9335 - val_loss: 0.6679 - val_accuracy: 0.8340\n",
      "Epoch 79/100\n",
      "4044/4044 [==============================] - 4s 960us/sample - loss: 0.1860 - accuracy: 0.9300 - val_loss: 0.6807 - val_accuracy: 0.8281\n",
      "Epoch 80/100\n",
      "4044/4044 [==============================] - 4s 1ms/sample - loss: 0.1919 - accuracy: 0.9332 - val_loss: 0.6709 - val_accuracy: 0.8409\n",
      "Epoch 81/100\n",
      "4044/4044 [==============================] - 4s 952us/sample - loss: 0.1994 - accuracy: 0.9256 - val_loss: 0.6404 - val_accuracy: 0.8538\n",
      "Epoch 82/100\n",
      "4044/4044 [==============================] - 4s 946us/sample - loss: 0.1908 - accuracy: 0.9330 - val_loss: 0.6427 - val_accuracy: 0.8567\n",
      "Epoch 83/100\n",
      "4044/4044 [==============================] - 4s 924us/sample - loss: 0.1985 - accuracy: 0.9275 - val_loss: 0.6596 - val_accuracy: 0.8498\n",
      "Epoch 84/100\n",
      "4044/4044 [==============================] - 4s 909us/sample - loss: 0.1956 - accuracy: 0.9310 - val_loss: 0.6375 - val_accuracy: 0.8478\n",
      "Epoch 85/100\n",
      "4044/4044 [==============================] - 4s 938us/sample - loss: 0.1862 - accuracy: 0.9332 - val_loss: 0.5826 - val_accuracy: 0.8597\n",
      "Epoch 86/100\n",
      "4044/4044 [==============================] - 4s 952us/sample - loss: 0.1771 - accuracy: 0.9377 - val_loss: 0.6397 - val_accuracy: 0.8478\n",
      "Epoch 87/100\n",
      "4044/4044 [==============================] - 4s 952us/sample - loss: 0.1958 - accuracy: 0.9285 - val_loss: 0.6208 - val_accuracy: 0.8557\n",
      "Epoch 88/100\n",
      "4044/4044 [==============================] - 4s 951us/sample - loss: 0.1979 - accuracy: 0.9285 - val_loss: 0.6163 - val_accuracy: 0.8547\n",
      "Epoch 89/100\n",
      "4044/4044 [==============================] - 4s 966us/sample - loss: 0.1835 - accuracy: 0.9342 - val_loss: 0.6613 - val_accuracy: 0.8419\n",
      "Epoch 90/100\n",
      "4044/4044 [==============================] - 4s 956us/sample - loss: 0.1910 - accuracy: 0.9298 - val_loss: 0.6947 - val_accuracy: 0.8340\n",
      "Epoch 91/100\n",
      "4044/4044 [==============================] - 4s 901us/sample - loss: 0.1842 - accuracy: 0.9320 - val_loss: 0.6821 - val_accuracy: 0.8360\n",
      "Epoch 92/100\n",
      "4044/4044 [==============================] - 4s 951us/sample - loss: 0.1867 - accuracy: 0.9318 - val_loss: 0.6900 - val_accuracy: 0.8389\n",
      "Epoch 93/100\n",
      "4044/4044 [==============================] - 4s 962us/sample - loss: 0.1741 - accuracy: 0.9362 - val_loss: 0.6361 - val_accuracy: 0.8429\n",
      "Epoch 94/100\n",
      "4044/4044 [==============================] - 4s 934us/sample - loss: 0.1742 - accuracy: 0.9347 - val_loss: 0.6588 - val_accuracy: 0.8429\n",
      "Epoch 95/100\n",
      "4044/4044 [==============================] - 4s 989us/sample - loss: 0.1731 - accuracy: 0.9337 - val_loss: 0.6257 - val_accuracy: 0.8528\n",
      "Epoch 96/100\n",
      "4044/4044 [==============================] - 4s 954us/sample - loss: 0.1834 - accuracy: 0.9332 - val_loss: 0.6786 - val_accuracy: 0.8379\n",
      "Epoch 97/100\n",
      "4044/4044 [==============================] - 4s 975us/sample - loss: 0.1711 - accuracy: 0.9421 - val_loss: 0.6640 - val_accuracy: 0.8449\n",
      "Epoch 98/100\n",
      "4044/4044 [==============================] - 4s 959us/sample - loss: 0.1796 - accuracy: 0.9347 - val_loss: 0.6526 - val_accuracy: 0.8498\n",
      "Epoch 99/100\n",
      "4044/4044 [==============================] - 4s 935us/sample - loss: 0.1738 - accuracy: 0.9355 - val_loss: 0.6346 - val_accuracy: 0.8508\n",
      "Epoch 100/100\n",
      "4044/4044 [==============================] - 4s 936us/sample - loss: 0.1710 - accuracy: 0.9367 - val_loss: 0.6435 - val_accuracy: 0.8468\n",
      "1265/1265 [==============================] - 0s 395us/sample - loss: 0.7660 - accuracy: 0.8119\n",
      "test_loss: 0.765951, accuracy: 0.811858\n"
     ]
    }
   ],
   "source": [
    "x_train = data.trainReviews\n",
    "y_train = data.trainLabels\n",
    "x_eval = data.evalReviews\n",
    "y_eval = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "n_symbols=data.n_symbols\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('./bilstm_attention/best_model/model_{epoch:02d}-{val_accuracy:.2f}.hdf5', save_best_only=True, save_weights_only=True)\n",
    "# history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.3,shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.2,shuffle=True)\n",
    "\n",
    "#验证\n",
    "\n",
    "scores = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "#保存模型\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./bilstm_attention/bilstm_attention.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "model.save_weights('./bilstm_attention/bilstm_attention.h5')\n",
    "\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
